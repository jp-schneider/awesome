{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from awesome.run.awesome_config import AwesomeConfig\n",
    "from awesome.run.awesome_runner import AwesomeRunner\n",
    "from awesome.util.reflection import class_name\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from awesome.model.convex_diffeomorphism_net import ConvexDiffeomorphismNet\n",
    "from awesome.util.path_tools import get_project_root_path\n",
    "from awesome.util.logging import basic_config\n",
    "from awesome.dataset.awesome_dataset import AwesomeDataset\n",
    "from awesome.dataset.fbms_sequence_dataset import FBMSSequenceDataset\n",
    "from awesome.measures.fbms_joint_loss import FBMSJointLoss\n",
    "from awesome.model.unet import UNet\n",
    "from awesome.measures.weighted_loss import WeightedLoss\n",
    "from awesome.measures.se import SE\n",
    "from awesome.measures.unaries_conversion_loss import UnariesConversionLoss\n",
    "from typing import Literal\n",
    "\n",
    "basic_config()\n",
    "\n",
    "os.chdir(get_project_root_path()) # Beeing in the root directory of the project is important for the relative paths to work consistently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.model.noisy_path_connected_net import NoisyPathConnectedNet\n",
    "from awesome.model.zoo import Zoo\n",
    "from awesome.model.net_factory import real_nvp_path_connected_net\n",
    "\n",
    "xytype = \"edge\"\n",
    "dataset_kind = \"train\"\n",
    "dataset = \"cars3\"\n",
    "all_frames = True\n",
    "segmentation_model_switch: Literal[\"original\", \"retrain\", \"retrain_xy\"] = \"original\"\n",
    "\n",
    "\n",
    "segmentation_model_state_dict_path = None\n",
    "if segmentation_model_switch == \"original\":\n",
    "    segmentation_model_state_dict_path = f\"./data/checkpoints/labels_with_uncertainty_flownet2_based/model_{dataset}_unet.pth\"\n",
    "elif segmentation_model_switch == \"retrain\":\n",
    "    segmentation_model_state_dict_path = f\"./data/checkpoints/refit_unet_uncertainty/23_11_13/model_{dataset}_unet.pth\"\n",
    "elif segmentation_model_switch == \"retrain_xy\":\n",
    "    segmentation_model_state_dict_path = f\"./data/checkpoints/refit_spatial_unet_uncertainty/23_11_13/model_{dataset}_unet.pth\"\n",
    "else:\n",
    "    raise ValueError(f\"Unknown segmentation_model_switch: {segmentation_model_switch}\")\n",
    "image_channel_format = \"bgr\" if segmentation_model_switch == \"original\" else \"rgb\"\n",
    "input_channels = 4 if xytype == \"edge\" else 6\n",
    "prior_criterion = UnariesConversionLoss(SE(reduction=\"mean\"))\n",
    "\n",
    "data_path = f\"./data/local_datasets/FBMS-59/{dataset_kind}/{dataset}\"\n",
    "\n",
    "real_dataset = FBMSSequenceDataset(\n",
    "                    dataset_path=data_path,\n",
    "                    weak_labels_dir = \"weak_labels/labels_with_uncertainty_flownet2_based\",\n",
    "                    processed_weak_labels_dir = \"weak_labels/labels_with_uncertainty_flownet2_based/processed\",\n",
    "                    confidence_dir= \"weak_labels/labels_with_uncertainty_flownet2_based/\",\n",
    "                    do_weak_label_preprocessing=True,\n",
    "                    do_uncertainty_label_flip=True,\n",
    "                    test_weak_label_integrity=False,\n",
    "                    all_frames=True,\n",
    "                )\n",
    "data_path = f\"./data/local_datasets/FBMS-59/{dataset_kind}/{dataset}\"\n",
    "\n",
    "batch_size = 2\n",
    "prior_epochs = 1000\n",
    "prior_reuse_state_epochs = 400\n",
    "prefit_flow_grid_epochs = 30\n",
    "prefit_convex_net_epochs = 400\n",
    "noisy_percentage = 1 / 3 \n",
    "\n",
    "cfg = AwesomeConfig(\n",
    "        name_experiment=f\"UNET+{dataset}+{xytype}+diffeo+only_prior+realnvp+spatio-temporal+noisy\",\n",
    "        dataset_type=class_name(AwesomeDataset),\n",
    "        dataset_args={\n",
    "            \"dataset\": real_dataset,\n",
    "            \"xytype\": xytype,\n",
    "            \"feature_dir\": f\"{data_path}/Feat\",\n",
    "            \"dimension\": \"3d\", # 2d for fcnet\n",
    "            \"mode\": \"model_input\",\n",
    "            \"model_input_requires_grad\": False,\n",
    "            \"batch_size\": 1,\n",
    "            \"split_ratio\": 1,\n",
    "            \"shuffle_in_dataloader\": False,\n",
    "            \"image_channel_format\": image_channel_format,\n",
    "            \"do_image_blurring\": True,\n",
    "            \"spatio_temporal\": True\n",
    "        },\n",
    "        segmentation_model_type=class_name(UNet),\n",
    "        segmentation_model_args={\n",
    "            'in_chn': input_channels,\n",
    "        },\n",
    "        segmentation_training_mode='multi',\n",
    "        segmentation_model_state_dict_path=segmentation_model_state_dict_path, # Path to the pretrained model\n",
    "        use_segmentation_output_inversion=True,\n",
    "        use_prior_model=True,\n",
    "        prior_model_args=dict(\n",
    "            channels=3,\n",
    "            hidden_units=32,\n",
    "            flow_n_flows=12,\n",
    "            flow_output_fn=\"tanh\",\n",
    "            norm=\"minmax\",\n",
    "            convex_net_hidden_units=130,\n",
    "            convex_net_hidden_layers=2,\n",
    "            network_type=NoisyPathConnectedNet,\n",
    "            \n",
    "        ),\n",
    "        prior_model_type=class_name(real_nvp_path_connected_net),\n",
    "        loss_type=class_name(FBMSJointLoss),\n",
    "        loss_args={\n",
    "            \"criterion\": WeightedLoss(torch.nn.BCELoss(), mode=\"sssdms\", noneclass=2),\n",
    "            \"alpha\": 1,\n",
    "            \"beta\": 1,\n",
    "        },\n",
    "        use_extra_penalty_hook=False, # Panalty hook for the panalty term that models output should match\n",
    "        #extra_penalty_after_n_epochs=1,\n",
    "        #use_reduce_lr_in_extra_penalty_hook=False,\n",
    "        use_lr_on_plateau_scheduler=False,\n",
    "        use_binary_classification=True, \n",
    "        num_epochs=100,\n",
    "        device=\"cuda\",\n",
    "        dtype=str(torch.float32),\n",
    "        runs_path=\"./runs/fbms_local/unet/noisy_spatio_temporal\",\n",
    "        optimizer_args={\n",
    "            \"lr\": 0.003,\n",
    "            \"betas\": (0.9, 0.999),\n",
    "            \"eps\": 1e-08,\n",
    "            \"amsgrad\": False\n",
    "        },\n",
    "        use_progress_bar=True,\n",
    "        plot_indices_during_training_nth_epoch=5,\n",
    "        plot_indices_during_training=real_dataset.get_ground_truth_indices(),\n",
    "        save_images_after_pretraining=True,\n",
    "        include_unaries_when_saving=True,\n",
    "        agent_args=dict(\n",
    "             do_pretraining=True,\n",
    "             pretrain_only=True, \n",
    "             force_pretrain=True,\n",
    "             pretrain_args=dict(\n",
    "                 use_pretrain_checkpoints=False,\n",
    "                 do_pretrain_checkpoints=False,\n",
    "                 lr=0.001,\n",
    "                 use_logger=True,\n",
    "                 use_step_logger=True,\n",
    "                 num_epochs=prior_epochs,\n",
    "                 proper_prior_fit_retrys=1,\n",
    "                 reuse_state_epochs=prior_reuse_state_epochs,\n",
    "                 # Prefit flow net identity => Flow will be identity(-like) at the beginning\n",
    "                 prefit_flow_net_identity=True,\n",
    "                 prefit_flow_net_identity_lr=1e-2,\n",
    "                 prefit_flow_net_identity_weight_decay=1e-5,\n",
    "                 prefit_flow_net_identity_num_epochs=prefit_flow_grid_epochs,\n",
    "                 # Prefit convex net, to start with a convex thing\n",
    "                 prefit_convex_net=True,\n",
    "                 prefit_convex_net_lr=1e-3,\n",
    "                 prefit_convex_net_weight_decay=0,\n",
    "                 prefit_convex_net_num_epochs=prefit_convex_net_epochs,\n",
    "                 batch_size=batch_size,\n",
    "                 noisy_percentage=noisy_percentage,\n",
    "                 zoo=Zoo()\n",
    "             )\n",
    "        ),\n",
    "        #output_folder=\"./runs/fbms_local/unet/TestUnet/\",\n",
    "    )\n",
    "# cfg.save_to_file(f\"./config/fbms_noisy_spatio_temporal/{cfg.name_experiment}.yaml\", \n",
    "#                  override=True, \n",
    "#                  make_dirs=True,\n",
    "#                  no_uuid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19:11:02:47.128 INFO     [tensorboard.py:55] Tensorboard logger created at: runs\\fbms_local\\unet\\noisy_spatio_temporal\\UNET+cars3+edge+diffeo+only_prior+realnvp+spatio-temporal+noisy_24_01_19_11_02_47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./runs/fbms_local/unet/noisy_spatio_temporal\\\\UNET+cars3+edge+diffeo+only_prior+realnvp+spatio-temporal+noisy_24_01_19_11_02_47\\\\init_cfg_awesome_config.yaml'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner = AwesomeRunner(cfg)\n",
    "runner.build()\n",
    "runner.store_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19:11:02:47.543 INFO     [torch_agent.py:581] Starting pretraining...\n",
      "2024-01-19:11:02:47.837 INFO     [path_connected_net.py:159] Loaded pretrained flow identity model!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf697a7d0a64d1597b55f45ec036cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Learning convex net:   0%|          | 0/400 [00:00<?, ?it/s, loss=2.692e-01]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4c96f7c5d74965bb996ebe9e15798c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pretraining epochs:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5cb97d735dc41f1b2706ad83bbec171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pretraining batches:  80%|########  | 8/10 [00:02<00:00,  3.96it/s, loss=0.224]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Schneider\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\awesome-dC4phDSK-py3.9\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:1013: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "2024-01-19:11:41:30.642 INFO     [torch_agent.py:595] Pretrain state saved to ./runs/fbms_local/unet/noisy_spatio_temporal\\UNET+cars3+edge+diffeo+only_prior+realnvp+spatio-temporal+noisy_24_01_19_11_02_47\\pretrain_state.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad01b2f3c244330bd7d4204e5a77e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating prior images...:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3b97ba23434d449b08a6a2fbbe956a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19:11:41:55.599 INFO     [handles.py:211] Saved checkpoint to ./runs/fbms_local/unet/noisy_spatio_temporal\\UNET+cars3+edge+diffeo+only_prior+realnvp+spatio-temporal+noisy_24_01_19_11_02_47\\checkpoint_epoch_0.pth at epoch 0\n",
      "2024-01-19:11:41:55.603 INFO     [torch_agent.py:614] Pretraining done!\n",
      "2024-01-19:11:41:55.604 INFO     [torch_agent.py:673] Pretraining done, exiting...\n"
     ]
    }
   ],
   "source": [
    "#runner.config.num_epochs = 2000\n",
    "runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.run.functions import get_result, split_model_result, plot_image_scribbles\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from awesome.run.functions import get_mpl_figure, plot_mask, prepare_input_eval\n",
    "from awesome.util.matplotlib import saveable\n",
    "import normflows as nf\n",
    "from typing import Optional, Tuple\n",
    "from awesome.model.path_connected_net import PathConnectedNet, minmax\n",
    "from matplotlib.axes import Axes\n",
    "\n",
    "def coordinate_grid(image_shape):\n",
    "    x = torch.arange(image_shape[1]).float()\n",
    "    y = torch.arange(image_shape[0]).float()\n",
    "    yy, xx = torch.meshgrid(y, x)\n",
    "    return torch.stack([yy, xx])\n",
    "\n",
    "def create_circle(image_shape: Tuple[int, int], radius: float, center: Tuple[float, float]):\n",
    "    grid = coordinate_grid(image_shape)\n",
    "    yy, xx = grid\n",
    "    circle = (yy - center[0])**2 + (xx - center[1])**2 <= radius**2\n",
    "    return circle.float()[None, ...]\n",
    "\n",
    "def subsample_mask(x,\n",
    "                   subsample: int = 25):\n",
    "    image_shape = x.shape[-2:]\n",
    "\n",
    "    ones_grid = torch.ones(x[0].shape)\n",
    "    subsampled_grid = torch.zeros(x[0].shape)\n",
    "    coords = (torch.argwhere(ones_grid) % subsample) == 0\n",
    "    coords_mask = coords.all(dim=-1).reshape((image_shape))\n",
    "    subsampled_grid[coords_mask] = 1\n",
    "    return subsampled_grid.bool()\n",
    "\n",
    "@saveable()\n",
    "def plot_output(img, \n",
    "                output, \n",
    "                target, \n",
    "                grid: torch.Tensor, \n",
    "                subsample:int = 25, \n",
    "                **kwargs):\n",
    "    image_shape = grid.shape[-2:]\n",
    "    def denorm_grid(grid):\n",
    "        image_shape = grid.shape[-2:]\n",
    "        grid_y = minmax(grid[0], grid[0].min(), grid[0].max(), 0, image_shape[0])\n",
    "        grid_x = minmax(grid[1], grid[1].min(), grid[1].max(), 0, image_shape[1])\n",
    "        grid_dnorm = torch.cat([grid_y[None, ...], grid_x[None, ...]], dim=0).detach().cpu().numpy()\n",
    "        return grid_dnorm\n",
    "\n",
    "    fig = plot_match(img, output, target, size=5, tight=True, subsample=subsample)\n",
    "    ax = fig.axes[0]\n",
    "    dnorm_grid_pt = denorm_grid(grid)\n",
    "\n",
    "    dnorm_grid_pt = torch.clamp(torch.tensor(dnorm_grid_pt), min=torch.tensor([[[0]], [[0]]]), max=torch.tensor([[[image_shape[0] - 1]], [[image_shape[1] - 1]]])).numpy()\n",
    "    \n",
    "    msk = subsample_mask(dnorm_grid_pt, subsample=subsample)\n",
    "\n",
    "    return plot_grid(dnorm_grid_pt, msk, ax, 'g', origin=\"lower\")\n",
    "    \n",
    "@saveable()\n",
    "def plot_grid(grid: torch.Tensor, \n",
    "              mask: torch.Tensor, \n",
    "              ax: Optional[Axes] = None, \n",
    "              tight: Optional[bool] = False,\n",
    "              size: Optional[float] = 5,\n",
    "              color: str = \"b\",\n",
    "              dense: bool = True,\n",
    "              origin: Literal['lower', 'upper'] = \"upper\"\n",
    "              ):\n",
    "    if ax is None:\n",
    "        fig, ax = get_mpl_figure(1, 1, tight=tight, size=size, ratio_or_img=grid)\n",
    "    else:\n",
    "        fig = ax.figure\n",
    "\n",
    "    dots = torch.argwhere(mask)\n",
    "\n",
    "    col_idx = torch.unique(dots[:, 0])\n",
    "    row_idx = torch.unique(dots[:, 1])\n",
    "\n",
    "    cols = grid[:, col_idx]\n",
    "    rows = grid[:, :, row_idx]\n",
    "\n",
    "    for idx in range(cols.shape[1]):\n",
    "        col = cols[:, idx]\n",
    "        if dense:\n",
    "            x = col[1]\n",
    "            y = col[0]\n",
    "        else:\n",
    "            x = col[1, row_idx]\n",
    "            y = col[0, row_idx]\n",
    "        ax.plot(x, y, color=color)\n",
    "\n",
    "    for idx in range(rows.shape[2]):\n",
    "        row = rows[:, :, idx]\n",
    "        if dense:\n",
    "            x = row[1]\n",
    "            y = row[0]\n",
    "        else:\n",
    "            x = row[1, col_idx]\n",
    "            y = row[0, col_idx]\n",
    "        ax.plot(x, y, color=color)\n",
    "\n",
    "    if origin == \"upper\":\n",
    "        ax.invert_yaxis()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_match(img, \n",
    "               output, \n",
    "               target, \n",
    "               subsample:int = 25, \n",
    "               grid: torch.Tensor = None,\n",
    "               **kwargs):\n",
    "\n",
    "    image_shape = img.shape[-2:]\n",
    "    if grid is None:\n",
    "        grid = coordinate_grid(image_shape)\n",
    "\n",
    "    subsampled_grid = torch.zeros_like(grid[0])\n",
    "\n",
    "    coords_mask = subsample_mask(grid, subsample=subsample)\n",
    "    subsampled_grid[coords_mask] = 1\n",
    "\n",
    "    add = []\n",
    "\n",
    "    if target is not None:\n",
    "        add.append(target)\n",
    "    if output is not None:\n",
    "        add.append(output)\n",
    "    \n",
    "    add.append(subsampled_grid.float()[None, ...])\n",
    "\n",
    "    stack_plot = torch.cat(add, dim=0)\n",
    "\n",
    "    fig = plot_mask(img, stack_plot, **kwargs)\n",
    "    return fig\n",
    "\n",
    "index = 0\n",
    "\n",
    "model = runner.agent._get_model()\n",
    "dataloader = runner.agent.training_dataset\n",
    "model_gets_targets = runner.agent.model_gets_targets\n",
    "p = os.path.join(runner.agent.agent_folder, \"pretrain_priors\")\n",
    "os.makedirs(p, exist_ok=True)\n",
    "\n",
    "#indices = [0, 19] #len(dataloader)\n",
    "for i in range(len(dataloader)):\n",
    "    res, ground_truth, img, fg, bg = get_result(model, dataloader, i, model_gets_targets=model_gets_targets)\n",
    "    res = split_model_result(res, model, dataloader, img)\n",
    "    res_prior = res.get(\"prior\", None)\n",
    "    res_pred = res[\"segmentation\"]\n",
    "    boxes = res.get(\"boxes\", None)\n",
    "    labels = res.get(\"labels\", None)\n",
    "    \n",
    "    iterations = 2000\n",
    "    fig = plot_image_scribbles(image=img,\n",
    "                        inference_result=res_pred,\n",
    "                        foreground_mask=fg,\n",
    "                        background_mask=bg,\n",
    "                        prior_result=res_prior,\n",
    "                        save=True,\n",
    "                        path=os.path.join(p, f\"prior_{i}_{iterations}.png\"),\n",
    "                        size=10,\n",
    "                        title=f\"Prior Epoch: {iterations}\", open=True)\n",
    "    #display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model.to(torch.device(\"cpu\"))\n",
    "        image, ground_truth, _input, targets, fg, bg, prior_state = prepare_input_eval(dataloader, model, index)\n",
    "        grid = model.prior_module.get_deformation(_input[2][None, ...])[0]\n",
    "\n",
    "    fig = plot_output(img, 1 - res_prior, 1 - res_pred, grid=grid, size=30, subsample=10,\n",
    "                    save=True,\n",
    "                        path=os.path.join(p, f\"deform_grid_{i}_{iterations}.png\"),\n",
    "                    )\n",
    "\n",
    "    #display(fig)\n",
    "    plt.close(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.run.functions import get_mpl_figure, plot_mask, prepare_input_eval\n",
    "from awesome.util.matplotlib import saveable\n",
    "import normflows as nf\n",
    "from typing import Optional, Tuple\n",
    "from awesome.model.path_connected_net import PathConnectedNet, minmax\n",
    "from matplotlib.axes import Axes\n",
    "\n",
    "def coordinate_grid(image_shape):\n",
    "    x = torch.arange(image_shape[1]).float()\n",
    "    y = torch.arange(image_shape[0]).float()\n",
    "    yy, xx = torch.meshgrid(y, x)\n",
    "    return torch.stack([yy, xx])\n",
    "\n",
    "def create_circle(image_shape: Tuple[int, int], radius: float, center: Tuple[float, float]):\n",
    "    grid = coordinate_grid(image_shape)\n",
    "    yy, xx = grid\n",
    "    circle = (yy - center[0])**2 + (xx - center[1])**2 <= radius**2\n",
    "    return circle.float()[None, ...]\n",
    "\n",
    "def subsample_mask(x,\n",
    "                   subsample: int = 25):\n",
    "    image_shape = x.shape[-2:]\n",
    "    ones_grid = torch.ones(x[0].shape)\n",
    "    subsampled_grid = torch.zeros(x[0].shape)\n",
    "    coords = (torch.argwhere(ones_grid) % subsample) == 0\n",
    "    coords_mask = coords.all(dim=-1).reshape((image_shape))\n",
    "    subsampled_grid[coords_mask] = 1\n",
    "    return subsampled_grid.bool()\n",
    "\n",
    "@saveable()\n",
    "def plot_output(img, \n",
    "                output, \n",
    "                target, \n",
    "                grid: torch.Tensor, \n",
    "                subsample:int = 25, \n",
    "                **kwargs):\n",
    "    image_shape = grid.shape[-2:]\n",
    "    def denorm_grid(grid):\n",
    "        image_shape = grid.shape[-2:]\n",
    "        grid_y = minmax(grid[0], grid[0].min(), grid[0].max(), 0, image_shape[0])\n",
    "        grid_x = minmax(grid[1], grid[1].min(), grid[1].max(), 0, image_shape[1])\n",
    "        grid_dnorm = torch.cat([grid_y[None, ...], grid_x[None, ...]], dim=0).detach().cpu().numpy()\n",
    "        return grid_dnorm\n",
    "\n",
    "    fig = plot_match(img, output, target, size=5, tight=True, subsample=subsample)\n",
    "    ax = fig.axes[0]\n",
    "    dnorm_grid_pt = denorm_grid(grid)\n",
    "\n",
    "    dnorm_grid_pt = torch.clamp(torch.tensor(dnorm_grid_pt), min=torch.tensor([[[0]], [[0]]]), max=torch.tensor([[[image_shape[0] - 1]], [[image_shape[1] - 1]]])).numpy()\n",
    "    \n",
    "    msk = subsample_mask(dnorm_grid_pt, subsample=subsample)\n",
    "\n",
    "    return plot_grid(dnorm_grid_pt, msk, ax, 'g', origin=\"lower\")\n",
    "    \n",
    "@saveable()\n",
    "def plot_grid(grid: torch.Tensor, \n",
    "              mask: torch.Tensor, \n",
    "              ax: Optional[Axes] = None, \n",
    "              tight: Optional[bool] = False,\n",
    "              size: Optional[float] = 5,\n",
    "              color: str = \"b\",\n",
    "              dense: bool = True,\n",
    "              origin: Literal['lower', 'upper'] = \"upper\"\n",
    "              ):\n",
    "    if ax is None:\n",
    "        fig, ax = get_mpl_figure(1, 1, tight=tight, size=size, ratio_or_img=grid)\n",
    "    else:\n",
    "        fig = ax.figure\n",
    "\n",
    "    dots = torch.argwhere(mask)\n",
    "\n",
    "    col_idx = torch.unique(dots[:, 0])\n",
    "    row_idx = torch.unique(dots[:, 1])\n",
    "\n",
    "    cols = grid[:, col_idx]\n",
    "    rows = grid[:, :, row_idx]\n",
    "\n",
    "    for idx in range(cols.shape[1]):\n",
    "        col = cols[:, idx]\n",
    "        if dense:\n",
    "            x = col[1]\n",
    "            y = col[0]\n",
    "        else:\n",
    "            x = col[1, row_idx]\n",
    "            y = col[0, row_idx]\n",
    "        ax.plot(x, y, color=color)\n",
    "\n",
    "    for idx in range(rows.shape[2]):\n",
    "        row = rows[:, :, idx]\n",
    "        if dense:\n",
    "            x = row[1]\n",
    "            y = row[0]\n",
    "        else:\n",
    "            x = row[1, col_idx]\n",
    "            y = row[0, col_idx]\n",
    "        ax.plot(x, y, color=color)\n",
    "\n",
    "    if origin == \"upper\":\n",
    "        ax.invert_yaxis()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_match(img, \n",
    "               output, \n",
    "               target, \n",
    "               subsample:int = 25, \n",
    "               grid: torch.Tensor = None,\n",
    "               **kwargs):\n",
    "\n",
    "    image_shape = img.shape[-2:]\n",
    "    if grid is None:\n",
    "        grid = coordinate_grid(image_shape)\n",
    "\n",
    "    subsampled_grid = torch.zeros_like(grid[0])\n",
    "\n",
    "    coords_mask = subsample_mask(grid, subsample=subsample)\n",
    "    subsampled_grid[coords_mask] = 1\n",
    "\n",
    "    add = []\n",
    "\n",
    "    if target is not None:\n",
    "        add.append(target)\n",
    "    if output is not None:\n",
    "        add.append(output)\n",
    "    \n",
    "    add.append(subsampled_grid.float()[None, ...])\n",
    "\n",
    "    stack_plot = torch.cat(add, dim=0)\n",
    "\n",
    "    fig = plot_mask(img, stack_plot, **kwargs)\n",
    "    return fig\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awesome-dC4phDSK-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

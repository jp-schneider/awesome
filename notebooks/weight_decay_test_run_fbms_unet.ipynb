{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from awesome.run.awesome_config import AwesomeConfig\n",
    "from awesome.run.awesome_runner import AwesomeRunner\n",
    "from awesome.util.reflection import class_name\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from awesome.dataset.sisbosi_dataset import SISBOSIDataset, ConvexityDataset as SISBOSIConvexityDataset\n",
    "from awesome.dataset.convexity_segmentation_dataset import ConvexitySegmentationDataset\n",
    "from awesome.measures.awesome_loss import AwesomeLoss\n",
    "from awesome.measures.regularizer_loss import RegularizerLoss\n",
    "from awesome.model.convex_diffeomorphism_net import ConvexDiffeomorphismNet\n",
    "from awesome.model.net import Net\n",
    "import awesome\n",
    "from awesome.util.path_tools import get_project_root_path\n",
    "from awesome.util.logging import basic_config\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "basic_config()\n",
    "\n",
    "os.chdir(get_project_root_path()) # Beeing in the root directory of the project is important for the relative paths to work consistently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from awesome.run.functions import get_result, plot_mask, prepare_input_eval, split_model_result, plot_image_scribbles\n",
    "from awesome.util.temporary_property import TemporaryProperty\n",
    "\n",
    "\n",
    "def create_wd_images(runner, model, dataloader, indices, path, copy_to = None):\n",
    "    for i in indices:\n",
    "        res, ground_truth, img, fg, bg = get_result(model, dataloader, i, model_gets_targets=False)\n",
    "        res = split_model_result(res, model, dataloader, img)\n",
    "\n",
    "        with TemporaryProperty(dataloader, do_image_blurring=False, image_channel_format=\"rgb\"):\n",
    "            hr_image = prepare_input_eval(dataloader, model, i)[0]\n",
    "\n",
    "        res_prior = res.get(\"prior\", None)\n",
    "        res_pred = res[\"segmentation\"]\n",
    "        boxes = res.get(\"boxes\", None)\n",
    "        labels = res.get(\"labels\", None)\n",
    "\n",
    "        p = os.path.join(path, \"weight_decay_images\")\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "\n",
    "        iterations = runner.config.agent_args.get(\"pretrain_args\").get(\"num_epochs\")\n",
    "        flow_wd = str(runner.config.agent_args.get(\"pretrain_args\").get(\"flow_weight_decay\")).replace(\".\", \"_\")\n",
    "\n",
    "        msk = torch.stack([res_pred, res_prior], dim=0)[:, 0,...]\n",
    "\n",
    "        img_path = os.path.join(p, f\"prior_{i}_it_{iterations}_wd_{flow_wd}.png\")\n",
    "                                \n",
    "        fig = plot_mask(image=hr_image,\n",
    "                        mask=msk,\n",
    "                        save=True,\n",
    "                        path=img_path,\n",
    "                        background_value=1,\n",
    "                        contour_linewidths=4,\n",
    "                        size=5,\n",
    "                        override=True,\n",
    "                        tight=True\n",
    "                        )\n",
    "        plt.close(fig)\n",
    "        \n",
    "        if copy_to is not None:\n",
    "            os.makedirs(copy_to, exist_ok=True)\n",
    "            shutil.copy(img_path, os.path.join(copy_to, os.path.basename(img_path)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from awesome.dataset.awesome_dataset import AwesomeDataset\n",
    "from awesome.dataset.fbms_sequence_dataset import FBMSSequenceDataset\n",
    "from awesome.dataset.sisbosi_dataset import SISBOSIDataset, ConvexityDataset as SISBOSIConvexityDataset\n",
    "from awesome.measures.awesome_image_loss_joint import AwesomeImageLossJoint\n",
    "from awesome.measures.awesome_image_loss import AwesomeImageLoss\n",
    "from awesome.measures.gradient_penalty_loss import GradientPenaltyLoss\n",
    "from awesome.measures.fbms_joint_loss import FBMSJointLoss\n",
    "from awesome.measures.regularizer_loss import RegularizerLoss\n",
    "from awesome.model.cnn_net import CNNNet\n",
    "from awesome.measures.tv import TV\n",
    "from awesome.model.convex_net import ConvexNet\n",
    "from awesome.model.net_factory import real_nvp_path_connected_net\n",
    "from awesome.model.unet import UNet\n",
    "from awesome.measures.weighted_loss import WeightedLoss\n",
    "from awesome.measures.se import SE\n",
    "from awesome.measures.ae import AE\n",
    "from awesome.measures.unaries_conversion_loss import UnariesConversionLoss\n",
    "from awesome.model.zoo import Zoo\n",
    "\n",
    "xytype = \"edge\"\n",
    "dataset_kind = \"train\"\n",
    "dataset = \"bear01\"\n",
    "all_frames = True\n",
    "subset = 0 #None #slice(0, 5)\n",
    "segmentation_model_switch: Literal[\"original\", \"retrain\", \"retrain_xy\"] = \"original\"\n",
    "\n",
    "\n",
    "segmentation_model_state_dict_path = None\n",
    "if segmentation_model_switch == \"original\":\n",
    "    segmentation_model_state_dict_path = f\"./data/checkpoints/labels_with_uncertainty_flownet2_based/model_{dataset}_unet.pth\"\n",
    "elif segmentation_model_switch == \"retrain\":\n",
    "    segmentation_model_state_dict_path = f\"./data/checkpoints/refit_unet_uncertainty/23_11_13/model_{dataset}_unet.pth\"\n",
    "elif segmentation_model_switch == \"retrain_xy\":\n",
    "    segmentation_model_state_dict_path = f\"./data/checkpoints/refit_spatial_unet_uncertainty/23_11_13/model_{dataset}_unet.pth\"\n",
    "else:\n",
    "    raise ValueError(f\"Unknown segmentation_model_switch: {segmentation_model_switch}\")\n",
    "image_channel_format = \"bgr\" if segmentation_model_switch == \"original\" else \"rgb\"\n",
    "\n",
    "prior_criterion = UnariesConversionLoss(SE(reduction=\"mean\"))\n",
    "\n",
    "data_path = f\"./data/local_datasets/FBMS-59/{dataset_kind}/{dataset}\"\n",
    "\n",
    "prior_epochs = 2000\n",
    "prior_refit_epochs = 1000\n",
    "\n",
    "eps = [2000, 4000]\n",
    "\n",
    "for prior_epochs in eps:\n",
    "    # [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 5e-2, 1e-2,]\n",
    "    for wd in [1e-6, 5e-6]:\n",
    "        real_dataset = FBMSSequenceDataset(\n",
    "                        dataset_path=data_path,\n",
    "                        weak_labels_dir = \"weak_labels/labels_with_uncertainty_flownet2_based\",\n",
    "                        processed_weak_labels_dir = \"weak_labels/labels_with_uncertainty_flownet2_based/processed\",\n",
    "                        confidence_dir= \"weak_labels/labels_with_uncertainty_flownet2_based/\",\n",
    "                        do_weak_label_preprocessing=True,\n",
    "                        do_uncertainty_label_flip=True,\n",
    "                        test_weak_label_integrity=False,\n",
    "                        all_frames=all_frames,\n",
    "                    )\n",
    "        \n",
    "        cfg = AwesomeConfig(\n",
    "                name_experiment=f\"WD_{wd}_UNET+{dataset}+{xytype}+diffeo+only_prior+TESTING\",\n",
    "                dataset_type=class_name(AwesomeDataset),\n",
    "                dataset_args={\n",
    "                    \"dataset\": real_dataset,\n",
    "                    \"xytype\": xytype,\n",
    "                    \"feature_dir\": f\"{data_path}/Feat\",\n",
    "                    \"dimension\": \"3d\", # 2d for fcnet\n",
    "                    \"mode\": \"model_input\",\n",
    "                    \"model_input_requires_grad\": False,\n",
    "                    \"batch_size\": 1,\n",
    "                    \"split_ratio\": 1,\n",
    "                    \"shuffle_in_dataloader\": False,\n",
    "                    \"image_channel_format\": image_channel_format,\n",
    "                    \"do_image_blurring\": True,\n",
    "                    \"subset\": subset\n",
    "                },\n",
    "                segmentation_model_type=class_name(UNet),\n",
    "                segmentation_model_args={\n",
    "                    'in_chn': 4 if segmentation_model_switch != \"retrain_xy\" else 6,\n",
    "                },\n",
    "                segmentation_training_mode='multi',\n",
    "                segmentation_model_state_dict_path=segmentation_model_state_dict_path, # Path to the pretrained model\n",
    "                use_segmentation_output_inversion=True,\n",
    "                use_prior_model=True,\n",
    "                prior_model_args=dict(\n",
    "                    channels=2,\n",
    "                    hidden_units=32,\n",
    "                    flow_n_flows=12,\n",
    "                    flow_output_fn=\"tanh\",\n",
    "                    norm=\"minmax\",\n",
    "                    convex_net_hidden_units=130,\n",
    "                    convex_net_hidden_layers=2,\n",
    "                ),\n",
    "                prior_model_type=class_name(real_nvp_path_connected_net),\n",
    "                loss_type=class_name(FBMSJointLoss),\n",
    "                loss_args={\n",
    "                    \"criterion\": WeightedLoss(torch.nn.BCELoss(), mode=\"sssdms\", noneclass=2),\n",
    "                    \"alpha\": 1,\n",
    "                    \"beta\": 1,\n",
    "                },\n",
    "                use_extra_penalty_hook=False, # Panalty hook for the panalty term that models output should match\n",
    "                #extra_penalty_after_n_epochs=1,\n",
    "                #use_reduce_lr_in_extra_penalty_hook=False,\n",
    "                use_lr_on_plateau_scheduler=False,\n",
    "                use_binary_classification=True, \n",
    "                num_epochs=100,\n",
    "                device=\"cuda\",\n",
    "                dtype=str(torch.float32),\n",
    "                runs_path=\"./runs/fbms_local/unet/weight_decay_experiments/\",\n",
    "                optimizer_args={\n",
    "                    \"lr\": 0.003,\n",
    "                    \"betas\": (0.9, 0.999),\n",
    "                    \"eps\": 1e-08,\n",
    "                    \"amsgrad\": False\n",
    "                },\n",
    "                use_progress_bar=True,\n",
    "                plot_indices_during_training_nth_epoch=5,\n",
    "                compute_metrics_during_training_nth_epoch=5,\n",
    "\n",
    "                save_images_after_pretraining=True,\n",
    "                include_unaries_when_saving=False,\n",
    "\n",
    "                plot_indices_during_training=real_dataset.get_ground_truth_indices(),\n",
    "                agent_args=dict(\n",
    "                    do_pretraining=True,\n",
    "                    pretrain_only=True, \n",
    "                    force_pretrain=True,\n",
    "                    pretrain_args=dict(\n",
    "                        use_pretrain_checkpoints=False,\n",
    "                        do_pretrain_checkpoints=False,\n",
    "                        lr=0.001,\n",
    "                        flow_weight_decay=wd,\n",
    "                        use_logger=True,\n",
    "                        use_step_logger=False,\n",
    "                        num_epochs=prior_epochs,\n",
    "                        proper_prior_fit_retrys=1,\n",
    "                        reuse_state_epochs=prior_refit_epochs,\n",
    "                        # Prefit flow net identity => Flow will be identity(-like) at the beginning\n",
    "                        prefit_flow_net_identity=True,\n",
    "                        prefit_flow_net_identity_lr=1e-2,\n",
    "                        prefit_flow_net_identity_weight_decay=1e-5,\n",
    "                        prefit_flow_net_identity_num_epochs=100,\n",
    "                        # Prefit convex net, to start with a convex thing\n",
    "                        prefit_convex_net=True,\n",
    "                        prefit_convex_net_lr=1e-3,\n",
    "                        prefit_convex_net_weight_decay=0,\n",
    "                        prefit_convex_net_num_epochs=200,\n",
    "                        zoo=Zoo()\n",
    "                    )\n",
    "                ),\n",
    "            )\n",
    "        \n",
    "        file = f\"./config/fbms_weight_decay/{cfg.name_experiment}.yaml\"\n",
    "        os.makedirs(os.path.dirname(file), exist_ok=True) \n",
    "        cfg.save_to_file(file, override=True, no_uuid=True)\n",
    "\n",
    "        runner = AwesomeRunner(cfg)\n",
    "        runner.build()\n",
    "        runner.store_config()\n",
    "        runner.train()\n",
    "\n",
    "        create_wd_images(runner, runner.agent._get_model(), runner.agent.training_dataset, [0], \"./runs/fbms_local/unet/weight_decay_experiments/summary_plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.analytics.result_model import ResultModel\n",
    "\n",
    "\n",
    "path = \"./runs/fbms_local/unet/weight_decay_experiments/WD_0.001_UNET+bear01+edge+diffeo+only_prior+TESTING_24_01_18_17_58_54\"\n",
    "result_model = ResultModel.from_path(path)\n",
    "runner = result_model.get_runner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.run.functions import get_result, plot_mask, prepare_input_eval, split_model_result, plot_image_scribbles\n",
    "from awesome.util.temporary_property import TemporaryProperty\n",
    "\n",
    "index = 0\n",
    "\n",
    "\n",
    "model = runner.agent._get_model()\n",
    "dataloader = runner.agent.training_dataset\n",
    "model_gets_targets = runner.agent.model_gets_targets\n",
    "\n",
    "indices = [0] #len(dataloader)\n",
    "\n",
    "create_wd_images(runner, model, dataloader, indices, path, copy_to = \"./runs/fbms_local/unet/weight_decay_experiments/summary_plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1e-2 * torch.randn(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.run.transforms2d import component_scale_matrix\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)\n",
    "lin = torch.nn.Linear(2, 2, bias=False)\n",
    "\n",
    "#lin.weight.data = component_scale_matrix(x=0.5, y=0.5)[:2, :2]\n",
    "\n",
    "x = torch.arange(1, 2.)\n",
    "y = torch.arange(1, 2.)\n",
    "xx, yy = torch.meshgrid(x, y)\n",
    "\n",
    "xx_shifted, yy_shifted = xx + 0.5, yy + 0.5\n",
    "\n",
    "xy = torch.stack([xx.flatten(), yy.flatten()], dim=1)\n",
    "\n",
    "xy_shifted = torch.stack([xx_shifted.flatten(), yy_shifted.flatten()], dim=1)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    xy_transf = lin(xy)\n",
    "\n",
    "X = np.hstack((xy_shifted.numpy(), np.ones((xy_shifted.shape[0], 1))))\n",
    "theta = np.linalg.pinv(X.T @ X) @ (X.T @ xy_transf.numpy())\n",
    "\n",
    "lin_new = torch.nn.Linear(2, 2, bias=True)\n",
    "lin_new.weight.data = torch.tensor(theta[:2, :].T, dtype=torch.float32)\n",
    "lin_new.bias.data = torch.tensor(theta[2, :], dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    xy_shifted_transf = lin_new(xy_shifted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.close(\"all\")\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "p1 = ax.plot(xy[:, 0], xy[:, 1], \"o\", label=\"Original\")\n",
    "p1 = ax.plot(xy_shifted[:, 0], xy_shifted[:, 1], \"o\", label=\"Shifted\")\n",
    "p2 = ax.plot(xy_transf[:, 0], xy_transf[:, 1], \"o\", label=\"Transformed\")\n",
    "p2 = ax.plot(xy_shifted_transf[:, 0], xy_shifted_transf[:, 1], \".\", label=\"Shifted Transformed\")\n",
    "\n",
    "plt.legend()\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = runner.agent._get_model()\n",
    "prior_model = model.prior_module\n",
    "state = prior_model.state_dict()\n",
    "\n",
    "torch.save(state, \"./temp/bear01_1_prior.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvexDiffeomorphismNet(130, 1, 4, 130)\n",
    "state = torch.load(\"./temp/bear01_1_prior.pth\", map_location=\"cpu\")\n",
    "model.load_state_dict(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.dataset.transformator import Transformator\n",
    "import torch\n",
    "grid = Transformator.get_positional_matrices(960, 540)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = torch.sigmoid(model(grid[None, ...]))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(out[0, 0, ...].numpy() < 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_center, x_center = grid.shape[1] // 2, grid.shape[2] // 2\n",
    "\n",
    "center = grid[:, y_center, x_center]\n",
    "center_2 = grid[:, y_center, x_center-3]\n",
    "center_3 = grid[:, y_center-3, x_center]\n",
    "\n",
    "center_right = grid[:, y_center, x_center + 50]\n",
    "center_right_2 = grid[:, y_center, x_center + 47]\n",
    "center_right_3 = grid[:, y_center-3, x_center + 50]\n",
    "\n",
    "center_points = torch.stack([center, center_2, center_3], dim=0)\n",
    "shifted_points = torch.stack([center_right, center_right_2, center_right_3], dim=0)\n",
    "\n",
    "#center_points = torch.flip(center_points, dims=[0, 1])\n",
    "#shifted_points = torch.flip(shifted_points, dims=[0, 1])\n",
    "\n",
    "\n",
    "# altered_model.translate(center, center_right)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     out_altered = torch.sigmoid(altered_model(grid[None, ...]))\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(out[0, 0, ...].numpy() < 0.5, label=\"Original\")\n",
    "# plt.imshow(out_altered[0, 0, ...].numpy() < 0.5, label=\"Altered\")\n",
    "# plt.show()\n",
    "\n",
    "t = torch.zeros_like(grid[0])\n",
    "\n",
    "for i in range(center_points.shape[0]):\n",
    "    t[(grid[0] == center_points[i, 0]) & (grid[1] == center_points[i, 1])] = 1\n",
    "\n",
    "for i in range(shifted_points.shape[0]):\n",
    "    t[(grid[0] == shifted_points[i, 0]) & (grid[1] == shifted_points[i, 1])] = 2\n",
    "\n",
    "plt.close(\"all\")\n",
    "plt.figure()\n",
    "plt.imshow(t)\n",
    "plt.xlim(x_center - 25, x_center + 75)\n",
    "plt.ylim(y_center - 25, y_center + 75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_right = grid[:, y_center, x_center + 50]\n",
    "center_right_x, center_right_y = center_right[0], center_right[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reshape(x: torch.Tensor):\n",
    "    v = x.permute(0, 2, 3, 1)\n",
    "    return v.reshape((v[..., :-1].numel(), v.shape[-1]))\n",
    "\n",
    "def _reshape_back(x: torch.Tensor):\n",
    "    v = x.reshape((b, h, w, -1))\n",
    "    return v.permute(0, 3, 1, 2)\n",
    "\n",
    "pixel_grid = _reshape(grid[None,...])\n",
    "pixel_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = pixel_grid.shape[0]\n",
    "pixel_grid[(bs // 2) + int((bs // 2) * 0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.run.transforms2d import component_scale_matrix\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "model.in_features = 2\n",
    "altered_model = copy.deepcopy(model)\n",
    "lin = altered_model.linear\n",
    "\n",
    "#lin.weight.data = component_scale_matrix(x=0.5, y=0.5)[:2, :2]\n",
    "\n",
    "def _reshape(x: torch.Tensor):\n",
    "    v = x.permute(0, 2, 3, 1)\n",
    "    return v.reshape((v[..., :-1].numel(), v.shape[-1]))\n",
    "\n",
    "def _reshape_back(x: torch.Tensor):\n",
    "    v = x.reshape((b, h, w, -1))\n",
    "    return v.permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "x = center_points\n",
    "x_shifted = shifted_points\n",
    "\n",
    "#x = torch.flip(center_points, dims=[0, 1])\n",
    "#x_shifted = torch.flip(shifted_points, dims=[0, 1])\n",
    "\n",
    "altered_model.translate_only_point(center, center_right)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_altered = torch.sigmoid(altered_model(grid[None, ...]))\n",
    "\n",
    "\n",
    "foreground = (1 - (out_altered > 0.5).float()).bool()[0,0]\n",
    "center_of_mass = torch.sum(torch.argwhere(foreground), dim=0) / torch.sum(foreground).to(dtype=torch.long)\n",
    "center_of_mass\n",
    "\n",
    "from awesome.run.functions import plot_mask_multi_channel, load_image\n",
    "masks = torch.cat([out, out_altered], dim=1)\n",
    "\n",
    "path = \"./data/local_datasets/FBMS-59/train/bear01/bear01_0001.jpg\"\n",
    "img = load_image(path)\n",
    "\n",
    "fig = plot_mask_multi_channel(img, masks[0])\n",
    "\n",
    "fig.axes[0].plot(center_of_mass[1], center_of_mass[0], \"o\", color=\"red\", markersize=5)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(out_altered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(torch.argwhere(out_altered.squeeze()), dim=0) / torch.sum(out_altered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_altered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.run.transforms2d import component_scale_matrix\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "altered_model = copy.deepcopy(model)\n",
    "lin = altered_model.linear\n",
    "\n",
    "#lin.weight.data = component_scale_matrix(x=0.5, y=0.5)[:2, :2]\n",
    "\n",
    "def _reshape(x: torch.Tensor):\n",
    "    v = x.permute(0, 2, 3, 1)\n",
    "    return v.reshape((v[..., :-1].numel(), v.shape[-1]))\n",
    "\n",
    "def _reshape_back(x: torch.Tensor):\n",
    "    v = x.reshape((b, h, w, -1))\n",
    "    return v.permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "x = center_points\n",
    "x_shifted = shifted_points\n",
    "\n",
    "#x = torch.flip(center_points, dims=[0, 1])\n",
    "#x_shifted = torch.flip(shifted_points, dims=[0, 1])\n",
    "\n",
    "\n",
    "altered_model.translate(x, x_shifted)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_altered = torch.sigmoid(altered_model(grid[None, ...]))\n",
    "\n",
    "from awesome.run.functions import plot_mask_multi_channel, load_image\n",
    "masks = torch.cat([out, out_altered], dim=1)\n",
    "\n",
    "path = \"./data/local_datasets/FBMS-59/train/bear01/bear01_0001.jpg\"\n",
    "img = load_image(path)\n",
    "\n",
    "plot_mask_multi_channel(img, masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.run.functions import plot_mask_multi_channel, load_image\n",
    "masks = torch.cat([out, out_altered], dim=1)\n",
    "\n",
    "path = \"./data/local_datasets/FBMS-59/train/bear01/bear01_0001.jpg\"\n",
    "img = load_image(path)\n",
    "\n",
    "plot_mask_multi_channel(img, masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.run.transforms2d import component_scale_matrix\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "altered_model = copy.deepcopy(model)\n",
    "lin = altered_model.linear\n",
    "\n",
    "#lin.weight.data = component_scale_matrix(x=0.5, y=0.5)[:2, :2]\n",
    "\n",
    "def _reshape(x: torch.Tensor):\n",
    "    v = x.permute(0, 2, 3, 1)\n",
    "    return v.reshape((v[..., :-1].numel(), v.shape[-1]))\n",
    "\n",
    "def _reshape_back(x: torch.Tensor):\n",
    "    v = x.reshape((b, h, w, -1))\n",
    "    return v.permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "x = center_points\n",
    "x_shifted = shifted_points\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_transf = lin(x)\n",
    "\n",
    "X = np.hstack((x_shifted.numpy(), np.ones((x_shifted.shape[0], 1))))\n",
    "theta = np.linalg.inv(X.T @ X) @ (X.T @ x_transf.numpy())\n",
    "\n",
    "\n",
    "weight = torch.tensor(theta[:-1, :].T, dtype=torch.float32)\n",
    "bias = torch.tensor(theta[-1, :], dtype=torch.float32)\n",
    "\n",
    "altered_model.linear.weight.data = weight\n",
    "altered_model.linear.bias.data = bias\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_altered = torch.sigmoid(altered_model(grid[None, ...]))\n",
    "\n",
    "from awesome.run.functions import plot_mask_multi_channel, load_image\n",
    "masks = torch.cat([out, out_altered], dim=1)\n",
    "\n",
    "path = \"./data/local_datasets/FBMS-59/train/bear01/bear01_0001.jpg\"\n",
    "img = load_image(path)\n",
    "\n",
    "plot_mask_multi_channel(img, masks[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = torch.cat([out, out_altered], dim=1)\n",
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.close(\"all\")\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "p1 = ax.plot(x_2[:, 0], x_2[:, 1], 0, \"o\", label=\"Original\")\n",
    "p1 = ax.plot(x_shifted2[:, 0], x_shifted2[:, 1], 0, \"o\", label=\"Shifted\")\n",
    "p2 = ax.plot(x_transf2[:, 0], x_transf2[:, 1],  x_transf2[:, 2], \"o\", label=\"Transformed\")\n",
    "p2 = ax.plot(x_shifted_transf2[:, 0], x_shifted_transf2[:, 1], x_shifted_transf2[:, 2], \".\", label=\"Shifted Transformed\")\n",
    "\n",
    "plt.legend()\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.dataset.prior_dataset import PriorManager\n",
    "from awesome.run.functions import prepare_input_eval\n",
    "from awesome.util.torch import TensorUtil\n",
    "model = runner.agent._get_model()\n",
    "dataloader = runner.agent.training_dataset\n",
    "model_gets_targets = runner.agent.model_gets_targets\n",
    "\n",
    "device = runner.agent.device\n",
    "\n",
    "image, ground_truth, _input, targets, fg, bg, prior_state = prepare_input_eval(dataloader, model, index)\n",
    "with torch.no_grad():\n",
    "    res = None\n",
    "    model.eval()\n",
    "    # Adding batch dimension as this would also do the batch composition\n",
    "    _input_d = TensorUtil.apply_deep(_input, lambda x: x[None, ...].to(device=device))\n",
    "    with PriorManager(model, prior_state, getattr(dataloader, \"__prior_cache__\", None)) as prior_manager:\n",
    "        # Patch the output model slidely\n",
    "        grid = _input[2] # Clean grid\n",
    "        center = grid[:, grid.shape[1] // 2, grid.shape[2] // 2: grid.shape[2] // 2 + 2].T.to(device=device)\n",
    "        center_right = grid[:, grid.shape[1] // 2, grid.shape[2] // 2 + 50: grid.shape[2] // 2 + 52].T.to(device=device)\n",
    "        \n",
    "        model.prior_module.translate(center, center_right)\n",
    "        model_kwargs = {}\n",
    "        if model_gets_targets:\n",
    "            targets = TensorUtil.apply_deep(targets, lambda x: x[None, ...].to(device=device))\n",
    "            model_kwargs['targets'] = targets\n",
    "\n",
    "        if isinstance(_input_d, (tuple, list)):\n",
    "            res = model(*_input_d, **model_kwargs)\n",
    "        else:\n",
    "            res = model(_input_d, **model_kwargs)\n",
    "        res = TensorUtil.apply_deep(res, lambda x: x.detach().cpu())\n",
    "    model.train()\n",
    "res = split_model_result(res, model, dataloader, img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.close(\"all\")\n",
    "plt.figure()\n",
    "plt.imshow(res[\"prior\"][0])\n",
    "#plt.imshow(res_old[\"prior\"][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_old = res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.run.transforms2d import component_scale_matrix\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "input_dim = 2\n",
    "out_dim = 3\n",
    "\n",
    "torch.manual_seed(1)\n",
    "lin = torch.nn.Linear(input_dim, out_dim, bias=True)\n",
    "\n",
    "#lin.weight.data = component_scale_matrix(x=0.5, y=0.5)[:2, :2]\n",
    "\n",
    "x = torch.randn(out_dim, input_dim)\n",
    "x_shifted = x + 0.5\n",
    "\n",
    "x_2 = torch.randn(5, input_dim)\n",
    "x_shifted2 = x_2 + 0.5\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_transf = lin(x)\n",
    "    x_transf2 = lin(x_2)\n",
    "\n",
    "X = np.hstack((x_shifted.numpy(), np.ones((x_shifted.shape[0], 1))))\n",
    "theta = np.linalg.inv(X.T @ X) @ (X.T @ x_transf.numpy())\n",
    "\n",
    "lin_new = torch.nn.Linear(input_dim, out_dim, bias=True)\n",
    "lin_new.weight.data = torch.tensor(theta[:-1, :].T, dtype=torch.float32)\n",
    "lin_new.bias.data = torch.tensor(theta[-1, :], dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_shifted_transf = lin_new(x_shifted)\n",
    "    x_shifted_transf2 = lin_new(x_shifted2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.close(\"all\")\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "p1 = ax.plot(x_2[:, 0], x_2[:, 1], 0, \"o\", label=\"Original\")\n",
    "p1 = ax.plot(x_shifted2[:, 0], x_shifted2[:, 1], 0, \"o\", label=\"Shifted\")\n",
    "p2 = ax.plot(x_transf2[:, 0], x_transf2[:, 1],  x_transf2[:, 2], \"o\", label=\"Transformed\")\n",
    "p2 = ax.plot(x_shifted_transf2[:, 0], x_shifted_transf2[:, 1], x_shifted_transf2[:, 2], \".\", label=\"Shifted Transformed\")\n",
    "\n",
    "plt.legend()\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((X, np.ones((X.shape[0], 1))))\n",
    "theta = np.linalg.solve(X.T @ X, X.T @ Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_transformation_matrix(x=0.5, y=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.run.transforms2d import component_transformation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(res_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(res_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.util.temporary_property import TemporaryProperty\n",
    "\n",
    "\n",
    "with TemporaryProperty(runner.dataloader, mode=\"sample\", return_prior=False):\n",
    "    fig = runner.dataloader[0][\"raw_sample\"].plot_weak_labels()\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = runner.dataloader.__dataset__\n",
    "import awesome.run.functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "index = 0\n",
    "\n",
    "size = 10\n",
    "cols = 4\n",
    "fig, ax = plt.subplots(1, cols, figsize=(size * cols, size))\n",
    "\n",
    "sample = dataloader[index]\n",
    "\n",
    "sample.plot(ax=ax[0], labels=sample.ground_truth_object_ids)\n",
    "ax[0].set_title(\"Ground truth\")\n",
    "\n",
    "sample.plot_weak_labels(ax=ax[1])\n",
    "ax[1].set_title(\"Weak labels\")\n",
    "\n",
    "sample.plot_selected_weak_labels(ax=ax[2])\n",
    "ax[2].set_title(\"Selected Weak labels\")\n",
    "\n",
    "sample.plot_selected(ax=ax[3], labels=[\"Foreground\", \"Background\"])\n",
    "ax[3].set_title(\"Selected ground truth\")\n",
    "\n",
    "\n",
    "mapping = sample._get_gt_object_id_weak_label_mapping()\n",
    "display(mapping)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"Stop here\"\n",
    "# Code for extracting the trajectories into the dataset folder\n",
    "tracks_path = \"data/local_datasets/FBMS-59/tracks\"\n",
    "dataset_dirs = \"data/local_datasets/FBMS-59/test/\"\n",
    "\n",
    "import shutil\n",
    "\n",
    "for folder in os.listdir(tracks_path):\n",
    "    inner_path = \"MulticutResults/pfldof0.5000004\"\n",
    "    complete_track_path = os.path.join(tracks_path, folder, inner_path)\n",
    "    tracks_file = list(os.listdir(complete_track_path))[0]\n",
    "    tracks_file_path = os.path.join(complete_track_path, tracks_file)\n",
    "\n",
    "    target_path = os.path.join(dataset_dirs, folder, \"tracks\", \"multicut\")\n",
    "    os.makedirs(target_path, exist_ok=True)\n",
    "    target_file_path = os.path.join(target_path, tracks_file)\n",
    "    shutil.copy(tracks_file_path, target_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awesome-dC4phDSK-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

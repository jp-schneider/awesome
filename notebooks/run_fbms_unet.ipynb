{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\projects\\AWESOME\\awesome\\agent\\torch_agent.py:19: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from awesome.run.awesome_config import AwesomeConfig\n",
    "from awesome.run.awesome_runner import AwesomeRunner\n",
    "from awesome.util.reflection import class_name\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from awesome.dataset.sisbosi_dataset import SISBOSIDataset, ConvexityDataset as SISBOSIConvexityDataset\n",
    "from awesome.dataset.convexity_segmentation_dataset import ConvexitySegmentationDataset\n",
    "from awesome.measures.awesome_loss import AwesomeLoss\n",
    "from awesome.measures.regularizer_loss import RegularizerLoss\n",
    "from awesome.model.convex_diffeomorphism_net import ConvexDiffeomorphismNet\n",
    "from awesome.model.net import Net\n",
    "import awesome\n",
    "from awesome.util.path_tools import get_project_root_path\n",
    "from awesome.util.logging import basic_config\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "basic_config()\n",
    "\n",
    "os.chdir(get_project_root_path()) # Beeing in the root directory of the project is important for the relative paths to work consistently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1cb2a9a493e4fff923f507715481467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading frames...:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9079d2e6e08411c991d6cd5d4bdefd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28:13:24:35.267 INFO     [tensorboard.py:55] Tensorboard logger created at: runs\\fbms_local\\unet\\prior_testing\\WD_5e-05_UNET+bear01+edge+diffeo+only_prior+TESTING_23_11_28_13_24_35\n",
      "2023-11-28:13:24:35.492 INFO     [torch_agent.py:559] Starting pretraining...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4240de2952bb4ee2b6b13aadc5feb305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pretraining images:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0da31611fa4b71b88fec2bb17523c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image 1: Pretraining:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\projects\\AWESOME\\notebooks\\run_fbms_unet.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/projects/AWESOME/notebooks/run_fbms_unet.ipynb#W1sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m runner\u001b[39m.\u001b[39mbuild()\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/projects/AWESOME/notebooks/run_fbms_unet.ipynb#W1sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m runner\u001b[39m.\u001b[39mstore_config()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/projects/AWESOME/notebooks/run_fbms_unet.ipynb#W1sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m runner\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mD:\\projects\\AWESOME\\awesome\\run\\awesome_runner.py:491\u001b[0m, in \u001b[0;36mAwesomeRunner.train\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 491\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mtrain(num_epochs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_epochs,\n\u001b[0;32m    492\u001b[0m                      tqdm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_progress_bar, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\projects\\AWESOME\\awesome\\agent\\torch_agent.py:645\u001b[0m, in \u001b[0;36mTorchAgent.train\u001b[1;34m(self, num_epochs, keep_device, **kwargs)\u001b[0m\n\u001b[0;32m    642\u001b[0m use_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtqdm\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogress_bar)\n\u001b[0;32m    644\u001b[0m \u001b[39m# Pretraining if wanted:\u001b[39;00m\n\u001b[1;32m--> 645\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pretrain(model, train_set\u001b[39m=\u001b[39mtrain_set, test_set\u001b[39m=\u001b[39mtest_set,\n\u001b[0;32m    646\u001b[0m                use_progress_bar\u001b[39m=\u001b[39muse_progress_bar, keep_device\u001b[39m=\u001b[39mkeep_device, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    650\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpretrain_only \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39mor\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpretrain_only \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mpretrain_only\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m))):\n\u001b[0;32m    651\u001b[0m     logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mPretraining done, exiting...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\projects\\AWESOME\\awesome\\agent\\torch_agent.py:560\u001b[0m, in \u001b[0;36mTorchAgent._pretrain\u001b[1;34m(self, model, train_set, test_set, use_progress_bar, keep_device, **kwargs)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m state_loaded \u001b[39mor\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforce_pretrain \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39mor\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforce_pretrain \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mforce_pretrain\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m))):\n\u001b[0;32m    559\u001b[0m     logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mStarting pretraining...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 560\u001b[0m     state \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpretrain(\n\u001b[0;32m    561\u001b[0m         train_set\u001b[39m=\u001b[39mtrain_set,\n\u001b[0;32m    562\u001b[0m         test_set\u001b[39m=\u001b[39mtest_set,\n\u001b[0;32m    563\u001b[0m         device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice,\n\u001b[0;32m    564\u001b[0m         agent\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[0;32m    565\u001b[0m         use_progress_bar\u001b[39m=\u001b[39muse_progress_bar,\n\u001b[0;32m    566\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpretraining_kwargs)\n\u001b[0;32m    567\u001b[0m     \u001b[39m# Save the state\u001b[39;00m\n\u001b[0;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\projects\\AWESOME\\awesome\\model\\wrapper_module.py:368\u001b[0m, in \u001b[0;36mWrapperModule.pretrain\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprior_module, PretrainableModule):\n\u001b[0;32m    367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPrior module must be a PretrainableModule\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 368\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprior_module\u001b[39m.\u001b[39mpretrain(\u001b[39m*\u001b[39margs, \n\u001b[0;32m    369\u001b[0m wrapper_module\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[0;32m    370\u001b[0m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\projects\\AWESOME\\awesome\\model\\convex_diffeomorphism_net.py:409\u001b[0m, in \u001b[0;36mConvexDiffeomorphismNet.pretrain\u001b[1;34m(self, train_set, test_set, device, agent, use_progress_bar, do_pretrain_checkpoints, use_pretrain_checkpoints, pretrain_checkpoint_dir, wrapper_module, **kwargs)\u001b[0m\n\u001b[0;32m    403\u001b[0m device_prior_output \u001b[39m=\u001b[39m wrapper_module\u001b[39m.\u001b[39mprocess_prior_output(\n\u001b[0;32m    404\u001b[0m     device_prior_output, use_sigmoid\u001b[39m=\u001b[39muse_prior_sigmoid)[\u001b[39mNone\u001b[39;00m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]  \u001b[39m# Add batch dim again\u001b[39;00m\n\u001b[0;32m    406\u001b[0m loss: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m criterion(\n\u001b[0;32m    407\u001b[0m     device_prior_output, unaries)\n\u001b[1;32m--> 409\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    410\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m    411\u001b[0m lr_scheduler\u001b[39m.\u001b[39mstep(loss)\n",
      "File \u001b[1;32mc:\\Users\\Schneider\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\awesome-dC4phDSK-py3.9\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Schneider\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\awesome-dC4phDSK-py3.9\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from awesome.dataset.awesome_dataset import AwesomeDataset\n",
    "from awesome.dataset.fbms_sequence_dataset import FBMSSequenceDataset\n",
    "from awesome.dataset.sisbosi_dataset import SISBOSIDataset, ConvexityDataset as SISBOSIConvexityDataset\n",
    "from awesome.measures.awesome_image_loss_joint import AwesomeImageLossJoint\n",
    "from awesome.measures.awesome_image_loss import AwesomeImageLoss\n",
    "from awesome.measures.gradient_penalty_loss import GradientPenaltyLoss\n",
    "from awesome.measures.fbms_joint_loss import FBMSJointLoss\n",
    "from awesome.measures.regularizer_loss import RegularizerLoss\n",
    "from awesome.model.cnn_net import CNNNet\n",
    "from awesome.measures.tv import TV\n",
    "from awesome.model.convex_net import ConvexNet\n",
    "from awesome.model.unet import UNet\n",
    "from awesome.measures.weighted_loss import WeightedLoss\n",
    "from awesome.measures.se import SE\n",
    "from awesome.measures.ae import AE\n",
    "from awesome.measures.unaries_conversion_loss import UnariesConversionLoss\n",
    "\n",
    "xytype = \"edge\"\n",
    "dataset_kind = \"train\"\n",
    "dataset = \"bear01\"\n",
    "all_frames = True\n",
    "subset = 0 #None #slice(0, 5)\n",
    "\n",
    "segmentation_model_switch: Literal[\"original\", \"retrain\", \"retrain_xy\"] = \"original\"\n",
    "\n",
    "\n",
    "segmentation_model_state_dict_path = None\n",
    "if segmentation_model_switch == \"original\":\n",
    "    segmentation_model_state_dict_path = f\"./data/checkpoints/labels_with_uncertainty_flownet2_based/model_{dataset}_unet.pth\"\n",
    "elif segmentation_model_switch == \"retrain\":\n",
    "    segmentation_model_state_dict_path = f\"./data/checkpoints/refit_unet_uncertainty/23_11_13/model_{dataset}_unet.pth\"\n",
    "elif segmentation_model_switch == \"retrain_xy\":\n",
    "    segmentation_model_state_dict_path = f\"./data/checkpoints/refit_spatial_unet_uncertainty/23_11_13/model_{dataset}_unet.pth\"\n",
    "else:\n",
    "    raise ValueError(f\"Unknown segmentation_model_switch: {segmentation_model_switch}\")\n",
    "image_channel_format = \"bgr\" if segmentation_model_switch == \"original\" else \"rgb\"\n",
    "\n",
    "prior_criterion = UnariesConversionLoss(SE(reduction=\"mean\"))\n",
    "\n",
    "data_path = f\"./data/local_datasets/FBMS-59/{dataset_kind}/{dataset}\"\n",
    "\n",
    "cfg = AwesomeConfig(\n",
    "        name_experiment=f\"GRAD_PAN_UNET+{dataset}+{xytype}+diffeo+only_prior+TESTING\",\n",
    "        dataset_type=class_name(AwesomeDataset),\n",
    "        dataset_args={\n",
    "            \"dataset\": FBMSSequenceDataset(\n",
    "                    dataset_path=data_path,\n",
    "                    weak_labels_dir = \"weak_labels/labels_with_uncertainty_flownet2_based\",\n",
    "                    processed_weak_labels_dir = \"weak_labels/labels_with_uncertainty_flownet2_based/processed\",\n",
    "                    confidence_dir= \"weak_labels/labels_with_uncertainty_flownet2_based/\",\n",
    "                    do_weak_label_preprocessing=True,\n",
    "                    do_uncertainty_label_flip=True,\n",
    "                    all_frames=all_frames\n",
    "                ),\n",
    "            \"xytype\": xytype,\n",
    "            \"feature_dir\": f\"{data_path}/Feat\",\n",
    "            \"dimension\": \"3d\", # 2d for fcnet\n",
    "            \"mode\": \"model_input\",\n",
    "            \"model_input_requires_grad\": False,\n",
    "            \"batch_size\": 1,\n",
    "            \"split_ratio\": 1,\n",
    "            \"shuffle_in_dataloader\": False,\n",
    "            \"image_channel_format\": image_channel_format,\n",
    "            \"do_image_blurring\": True,\n",
    "            \"subset\": subset\n",
    "        },\n",
    "        segmentation_model_type=class_name(UNet),\n",
    "        segmentation_model_args={\n",
    "            'in_chn': 4 if segmentation_model_switch != \"retrain_xy\" else 6,\n",
    "        },\n",
    "        segmentation_training_mode='multi',\n",
    "        segmentation_model_state_dict_path=segmentation_model_state_dict_path, # Path to the pretrained model\n",
    "        use_segmentation_output_inversion=True,\n",
    "        use_prior_model=True,\n",
    "        prior_model_args=dict(\n",
    "            n_hidden=130,\n",
    "            n_hidden_layers=2,\n",
    "            diffeo_args=dict(\n",
    "                num_coupling=6,\n",
    "                width=130,\n",
    "                backbone=\"residual_block\"\n",
    "            ),\n",
    "        ),\n",
    "        prior_model_type=class_name(ConvexDiffeomorphismNet),\n",
    "        loss_type=class_name(FBMSJointLoss),\n",
    "        loss_args={\n",
    "            \"criterion\": WeightedLoss(torch.nn.BCELoss(), mode=\"sssdms\", noneclass=2),\n",
    "            \"alpha\": 1,\n",
    "            \"beta\": 1,\n",
    "        },\n",
    "        use_extra_penalty_hook=False, # Panalty hook for the panalty term that models output should match\n",
    "        #extra_penalty_after_n_epochs=1,\n",
    "        #use_reduce_lr_in_extra_penalty_hook=False,\n",
    "        use_lr_on_plateau_scheduler=False,\n",
    "        use_binary_classification=True, \n",
    "        num_epochs=100,\n",
    "        device=\"cuda\",\n",
    "        dtype=str(torch.float32),\n",
    "        runs_path=\"./runs/fbms_local/unet/prior_testing/\",\n",
    "        optimizer_args={\n",
    "            \"lr\": 0.003,\n",
    "            \"betas\": (0.9, 0.999),\n",
    "            \"eps\": 1e-08,\n",
    "            \"amsgrad\": False\n",
    "        },\n",
    "        use_progress_bar=True,\n",
    "        semantic_soft_segmentation_code_dir=\"../siggraph/\",\n",
    "        semantic_soft_segmentation_model_checkpoint_dir=\"./data/sss_checkpoint/model\",\n",
    "        plot_indices_during_training_nth_epoch=10,\n",
    "        plot_indices_during_training=[0, 19, 39, 59, 79, 99],\n",
    "        agent_args=dict(\n",
    "            do_pretraining=True,\n",
    "            pretrain_only=True, \n",
    "            force_pretrain=True,\n",
    "            pretrain_args=dict(\n",
    "                lr=0.001,\n",
    "                use_logger=True,\n",
    "                use_step_logger=False,\n",
    "                num_epochs=2000,\n",
    "                proper_prior_fit_retrys=0,\n",
    "                criterion=prior_criterion,\n",
    "                do_pretrain_checkpoints=False,\n",
    "                use_pretrain_checkpoints=False,\n",
    "                weight_decay_on_weight_g=5e-5,\n",
    "            )\n",
    "        ),\n",
    "        weight_decay_on_weight_norm_modules=5e-5,\n",
    "        #output_folder=\"./runs/fbms_local/unet/TestUnet/\",\n",
    "    )\n",
    "#cfg.save_to_file(f\"./config/{cfg.name_experiment}.yaml\", override=True, no_uuid=True)\n",
    "\n",
    "runner = AwesomeRunner(cfg)\n",
    "runner.build()\n",
    "runner.store_config()\n",
    "\n",
    "runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6afe1637f0e491e916ba826cd5bb920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16:17:36:36.134 INFO     [tensorboard.py:55] Tensorboard logger created at: runs\\fbms_local\\unet\\prior_testing\\WD_1e-4_UNET+bear01+edge+diffeo+only_prior+TESTING_23_11_16_17_36_36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./runs/fbms_local/unet/prior_testing/WD_1e-4_UNET+bear01+edge+diffeo+only_prior+TESTING_23_11_16_17_36_36\\\\init_cfg_awesome_config.yaml'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner = AwesomeRunner(cfg)\n",
    "runner.build()\n",
    "runner.store_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16:17:36:37.531 INFO     [torch_agent.py:559] Starting pretraining...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc5ae8b9a4e48c3958e2bd9fab1105f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pretraining images:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e77af4f3aa74dae8e63270361fc2bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image 1: Pretraining:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00203: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 00203: reducing learning rate of group 1 to 5.0000e-04.\n",
      "Epoch 00404: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 00404: reducing learning rate of group 1 to 2.5000e-04.\n",
      "Epoch 00605: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 00605: reducing learning rate of group 1 to 1.2500e-04.\n",
      "Epoch 00806: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 00806: reducing learning rate of group 1 to 6.2500e-05.\n",
      "Epoch 01007: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 01007: reducing learning rate of group 1 to 3.1250e-05.\n",
      "Epoch 01208: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 01208: reducing learning rate of group 1 to 1.5625e-05.\n",
      "Epoch 01409: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch 01409: reducing learning rate of group 1 to 7.8125e-06.\n",
      "Epoch 01610: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch 01610: reducing learning rate of group 1 to 3.9063e-06.\n",
      "Epoch 01811: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch 01811: reducing learning rate of group 1 to 1.9531e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16:17:39:46.472 INFO     [convex_diffeomorphism_net.py:440] Prior fit not proper. Retries exceeded. Metric: 0.0 Threshold: 0.5\n",
      "2023-11-16:17:39:46.495 INFO     [torch_agent.py:573] Pretrain state saved to ./runs/fbms_local/unet/prior_testing/WD_1e-4_UNET+bear01+edge+diffeo+only_prior+TESTING_23_11_16_17_36_36\\pretrain_state.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772de0f944894c2ca4128b5d953cebea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating prior images...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d65848202f6462a87f25c0deef30e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing metrics...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16:17:39:48.398 INFO     [handles.py:211] Saved checkpoint to ./runs/fbms_local/unet/prior_testing/WD_1e-4_UNET+bear01+edge+diffeo+only_prior+TESTING_23_11_16_17_36_36\\checkpoint_epoch_0.pth at epoch 0\n",
      "2023-11-16:17:39:48.408 INFO     [torch_agent.py:592] Pretraining done!\n",
      "2023-11-16:17:39:48.409 INFO     [torch_agent.py:651] Pretraining done, exiting...\n"
     ]
    }
   ],
   "source": [
    "#runner.config.num_epochs = 2000\n",
    "runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.agent.open_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.run.functions import get_result, split_model_result, plot_image_scribbles\n",
    "\n",
    "index = 0\n",
    "\n",
    "model = runner.agent._get_model()\n",
    "dataloader = runner.agent.training_dataset\n",
    "model_gets_targets = runner.agent.model_gets_targets\n",
    "\n",
    "figs = []\n",
    "indices = [0, 19] #len(dataloader)\n",
    "for i in indices:\n",
    "    res, ground_truth, img, fg, bg = get_result(model, dataloader, i, model_gets_targets=model_gets_targets)\n",
    "    res = split_model_result(res, model, dataloader, img)\n",
    "    res_prior = res.get(\"prior\", None)\n",
    "    res_pred = res[\"segmentation\"]\n",
    "    boxes = res.get(\"boxes\", None)\n",
    "    labels = res.get(\"labels\", None)\n",
    "\n",
    "    p = os.path.join(runner.agent.agent_folder, \"pretrain_priors\")\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "    iterations = 2000\n",
    "    fig = plot_image_scribbles(image=img,\n",
    "                        inference_result=res_pred,\n",
    "                        foreground_mask=fg,\n",
    "                        background_mask=bg,\n",
    "                        prior_result=res_prior,\n",
    "                        save=True,\n",
    "                        path=os.path.join(p, f\"prior_{i}_{iterations}.png\"),\n",
    "                        size=10,\n",
    "                        title=f\"Prior Epoch: {iterations}\", open=True)\n",
    "    figs.append(fig)\n",
    "\n",
    "figs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1e-2 * torch.randn(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.run.transforms2d import component_scale_matrix\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)\n",
    "lin = torch.nn.Linear(2, 2, bias=False)\n",
    "\n",
    "#lin.weight.data = component_scale_matrix(x=0.5, y=0.5)[:2, :2]\n",
    "\n",
    "x = torch.arange(1, 2.)\n",
    "y = torch.arange(1, 2.)\n",
    "xx, yy = torch.meshgrid(x, y)\n",
    "\n",
    "xx_shifted, yy_shifted = xx + 0.5, yy + 0.5\n",
    "\n",
    "xy = torch.stack([xx.flatten(), yy.flatten()], dim=1)\n",
    "\n",
    "xy_shifted = torch.stack([xx_shifted.flatten(), yy_shifted.flatten()], dim=1)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    xy_transf = lin(xy)\n",
    "\n",
    "X = np.hstack((xy_shifted.numpy(), np.ones((xy_shifted.shape[0], 1))))\n",
    "theta = np.linalg.pinv(X.T @ X) @ (X.T @ xy_transf.numpy())\n",
    "\n",
    "lin_new = torch.nn.Linear(2, 2, bias=True)\n",
    "lin_new.weight.data = torch.tensor(theta[:2, :].T, dtype=torch.float32)\n",
    "lin_new.bias.data = torch.tensor(theta[2, :], dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    xy_shifted_transf = lin_new(xy_shifted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.close(\"all\")\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "p1 = ax.plot(xy[:, 0], xy[:, 1], \"o\", label=\"Original\")\n",
    "p1 = ax.plot(xy_shifted[:, 0], xy_shifted[:, 1], \"o\", label=\"Shifted\")\n",
    "p2 = ax.plot(xy_transf[:, 0], xy_transf[:, 1], \"o\", label=\"Transformed\")\n",
    "p2 = ax.plot(xy_shifted_transf[:, 0], xy_shifted_transf[:, 1], \".\", label=\"Shifted Transformed\")\n",
    "\n",
    "plt.legend()\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = runner.agent._get_model()\n",
    "prior_model = model.prior_module\n",
    "state = prior_model.state_dict()\n",
    "\n",
    "torch.save(state, \"./temp/bear01_1_prior.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvexDiffeomorphismNet(130, 1, 4, 130)\n",
    "state = torch.load(\"./temp/bear01_1_prior.pth\", map_location=\"cpu\")\n",
    "model.load_state_dict(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.dataset.transformator import Transformator\n",
    "import torch\n",
    "grid = Transformator.get_positional_matrices(960, 540)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = torch.sigmoid(model(grid[None, ...]))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(out[0, 0, ...].numpy() < 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_center, x_center = grid.shape[1] // 2, grid.shape[2] // 2\n",
    "\n",
    "center = grid[:, y_center, x_center]\n",
    "center_2 = grid[:, y_center, x_center-3]\n",
    "center_3 = grid[:, y_center-3, x_center]\n",
    "\n",
    "center_right = grid[:, y_center, x_center + 50]\n",
    "center_right_2 = grid[:, y_center, x_center + 47]\n",
    "center_right_3 = grid[:, y_center-3, x_center + 50]\n",
    "\n",
    "center_points = torch.stack([center, center_2, center_3], dim=0)\n",
    "shifted_points = torch.stack([center_right, center_right_2, center_right_3], dim=0)\n",
    "\n",
    "#center_points = torch.flip(center_points, dims=[0, 1])\n",
    "#shifted_points = torch.flip(shifted_points, dims=[0, 1])\n",
    "\n",
    "\n",
    "# altered_model.translate(center, center_right)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     out_altered = torch.sigmoid(altered_model(grid[None, ...]))\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(out[0, 0, ...].numpy() < 0.5, label=\"Original\")\n",
    "# plt.imshow(out_altered[0, 0, ...].numpy() < 0.5, label=\"Altered\")\n",
    "# plt.show()\n",
    "\n",
    "t = torch.zeros_like(grid[0])\n",
    "\n",
    "for i in range(center_points.shape[0]):\n",
    "    t[(grid[0] == center_points[i, 0]) & (grid[1] == center_points[i, 1])] = 1\n",
    "\n",
    "for i in range(shifted_points.shape[0]):\n",
    "    t[(grid[0] == shifted_points[i, 0]) & (grid[1] == shifted_points[i, 1])] = 2\n",
    "\n",
    "plt.close(\"all\")\n",
    "plt.figure()\n",
    "plt.imshow(t)\n",
    "plt.xlim(x_center - 25, x_center + 75)\n",
    "plt.ylim(y_center - 25, y_center + 75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_right = grid[:, y_center, x_center + 50]\n",
    "center_right_x, center_right_y = center_right[0], center_right[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reshape(x: torch.Tensor):\n",
    "    v = x.permute(0, 2, 3, 1)\n",
    "    return v.reshape((v[..., :-1].numel(), v.shape[-1]))\n",
    "\n",
    "def _reshape_back(x: torch.Tensor):\n",
    "    v = x.reshape((b, h, w, -1))\n",
    "    return v.permute(0, 3, 1, 2)\n",
    "\n",
    "pixel_grid = _reshape(grid[None,...])\n",
    "pixel_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = pixel_grid.shape[0]\n",
    "pixel_grid[(bs // 2) + int((bs // 2) * 0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.run.transforms2d import component_scale_matrix\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "model.in_features = 2\n",
    "altered_model = copy.deepcopy(model)\n",
    "lin = altered_model.linear\n",
    "\n",
    "#lin.weight.data = component_scale_matrix(x=0.5, y=0.5)[:2, :2]\n",
    "\n",
    "def _reshape(x: torch.Tensor):\n",
    "    v = x.permute(0, 2, 3, 1)\n",
    "    return v.reshape((v[..., :-1].numel(), v.shape[-1]))\n",
    "\n",
    "def _reshape_back(x: torch.Tensor):\n",
    "    v = x.reshape((b, h, w, -1))\n",
    "    return v.permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "x = center_points\n",
    "x_shifted = shifted_points\n",
    "\n",
    "#x = torch.flip(center_points, dims=[0, 1])\n",
    "#x_shifted = torch.flip(shifted_points, dims=[0, 1])\n",
    "\n",
    "altered_model.translate_only_point(center, center_right)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_altered = torch.sigmoid(altered_model(grid[None, ...]))\n",
    "\n",
    "\n",
    "foreground = (1 - (out_altered > 0.5).float()).bool()[0,0]\n",
    "center_of_mass = torch.sum(torch.argwhere(foreground), dim=0) / torch.sum(foreground).to(dtype=torch.long)\n",
    "center_of_mass\n",
    "\n",
    "from awesome.run.functions import plot_mask_multi_channel, load_image\n",
    "masks = torch.cat([out, out_altered], dim=1)\n",
    "\n",
    "path = \"./data/local_datasets/FBMS-59/train/bear01/bear01_0001.jpg\"\n",
    "img = load_image(path)\n",
    "\n",
    "fig = plot_mask_multi_channel(img, masks[0])\n",
    "\n",
    "fig.axes[0].plot(center_of_mass[1], center_of_mass[0], \"o\", color=\"red\", markersize=5)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(out_altered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(torch.argwhere(out_altered.squeeze()), dim=0) / torch.sum(out_altered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_altered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.run.transforms2d import component_scale_matrix\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "altered_model = copy.deepcopy(model)\n",
    "lin = altered_model.linear\n",
    "\n",
    "#lin.weight.data = component_scale_matrix(x=0.5, y=0.5)[:2, :2]\n",
    "\n",
    "def _reshape(x: torch.Tensor):\n",
    "    v = x.permute(0, 2, 3, 1)\n",
    "    return v.reshape((v[..., :-1].numel(), v.shape[-1]))\n",
    "\n",
    "def _reshape_back(x: torch.Tensor):\n",
    "    v = x.reshape((b, h, w, -1))\n",
    "    return v.permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "x = center_points\n",
    "x_shifted = shifted_points\n",
    "\n",
    "#x = torch.flip(center_points, dims=[0, 1])\n",
    "#x_shifted = torch.flip(shifted_points, dims=[0, 1])\n",
    "\n",
    "\n",
    "altered_model.translate(x, x_shifted)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_altered = torch.sigmoid(altered_model(grid[None, ...]))\n",
    "\n",
    "from awesome.run.functions import plot_mask_multi_channel, load_image\n",
    "masks = torch.cat([out, out_altered], dim=1)\n",
    "\n",
    "path = \"./data/local_datasets/FBMS-59/train/bear01/bear01_0001.jpg\"\n",
    "img = load_image(path)\n",
    "\n",
    "plot_mask_multi_channel(img, masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.run.functions import plot_mask_multi_channel, load_image\n",
    "masks = torch.cat([out, out_altered], dim=1)\n",
    "\n",
    "path = \"./data/local_datasets/FBMS-59/train/bear01/bear01_0001.jpg\"\n",
    "img = load_image(path)\n",
    "\n",
    "plot_mask_multi_channel(img, masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.run.transforms2d import component_scale_matrix\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "altered_model = copy.deepcopy(model)\n",
    "lin = altered_model.linear\n",
    "\n",
    "#lin.weight.data = component_scale_matrix(x=0.5, y=0.5)[:2, :2]\n",
    "\n",
    "def _reshape(x: torch.Tensor):\n",
    "    v = x.permute(0, 2, 3, 1)\n",
    "    return v.reshape((v[..., :-1].numel(), v.shape[-1]))\n",
    "\n",
    "def _reshape_back(x: torch.Tensor):\n",
    "    v = x.reshape((b, h, w, -1))\n",
    "    return v.permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "x = center_points\n",
    "x_shifted = shifted_points\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_transf = lin(x)\n",
    "\n",
    "X = np.hstack((x_shifted.numpy(), np.ones((x_shifted.shape[0], 1))))\n",
    "theta = np.linalg.inv(X.T @ X) @ (X.T @ x_transf.numpy())\n",
    "\n",
    "\n",
    "weight = torch.tensor(theta[:-1, :].T, dtype=torch.float32)\n",
    "bias = torch.tensor(theta[-1, :], dtype=torch.float32)\n",
    "\n",
    "altered_model.linear.weight.data = weight\n",
    "altered_model.linear.bias.data = bias\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_altered = torch.sigmoid(altered_model(grid[None, ...]))\n",
    "\n",
    "from awesome.run.functions import plot_mask_multi_channel, load_image\n",
    "masks = torch.cat([out, out_altered], dim=1)\n",
    "\n",
    "path = \"./data/local_datasets/FBMS-59/train/bear01/bear01_0001.jpg\"\n",
    "img = load_image(path)\n",
    "\n",
    "plot_mask_multi_channel(img, masks[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = torch.cat([out, out_altered], dim=1)\n",
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.close(\"all\")\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "p1 = ax.plot(x_2[:, 0], x_2[:, 1], 0, \"o\", label=\"Original\")\n",
    "p1 = ax.plot(x_shifted2[:, 0], x_shifted2[:, 1], 0, \"o\", label=\"Shifted\")\n",
    "p2 = ax.plot(x_transf2[:, 0], x_transf2[:, 1],  x_transf2[:, 2], \"o\", label=\"Transformed\")\n",
    "p2 = ax.plot(x_shifted_transf2[:, 0], x_shifted_transf2[:, 1], x_shifted_transf2[:, 2], \".\", label=\"Shifted Transformed\")\n",
    "\n",
    "plt.legend()\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.dataset.prior_dataset import PriorManager\n",
    "from awesome.run.functions import prepare_input_eval\n",
    "from awesome.util.torch import TensorUtil\n",
    "model = runner.agent._get_model()\n",
    "dataloader = runner.agent.training_dataset\n",
    "model_gets_targets = runner.agent.model_gets_targets\n",
    "\n",
    "device = runner.agent.device\n",
    "\n",
    "image, ground_truth, _input, targets, fg, bg, prior_state = prepare_input_eval(dataloader, model, index)\n",
    "with torch.no_grad():\n",
    "    res = None\n",
    "    model.eval()\n",
    "    # Adding batch dimension as this would also do the batch composition\n",
    "    _input_d = TensorUtil.apply_deep(_input, lambda x: x[None, ...].to(device=device))\n",
    "    with PriorManager(model, prior_state, getattr(dataloader, \"__prior_cache__\", None)) as prior_manager:\n",
    "        # Patch the output model slidely\n",
    "        grid = _input[2] # Clean grid\n",
    "        center = grid[:, grid.shape[1] // 2, grid.shape[2] // 2: grid.shape[2] // 2 + 2].T.to(device=device)\n",
    "        center_right = grid[:, grid.shape[1] // 2, grid.shape[2] // 2 + 50: grid.shape[2] // 2 + 52].T.to(device=device)\n",
    "        \n",
    "        model.prior_module.translate(center, center_right)\n",
    "        model_kwargs = {}\n",
    "        if model_gets_targets:\n",
    "            targets = TensorUtil.apply_deep(targets, lambda x: x[None, ...].to(device=device))\n",
    "            model_kwargs['targets'] = targets\n",
    "\n",
    "        if isinstance(_input_d, (tuple, list)):\n",
    "            res = model(*_input_d, **model_kwargs)\n",
    "        else:\n",
    "            res = model(_input_d, **model_kwargs)\n",
    "        res = TensorUtil.apply_deep(res, lambda x: x.detach().cpu())\n",
    "    model.train()\n",
    "res = split_model_result(res, model, dataloader, img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.close(\"all\")\n",
    "plt.figure()\n",
    "plt.imshow(res[\"prior\"][0])\n",
    "#plt.imshow(res_old[\"prior\"][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_old = res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.run.transforms2d import component_scale_matrix\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "input_dim = 2\n",
    "out_dim = 3\n",
    "\n",
    "torch.manual_seed(1)\n",
    "lin = torch.nn.Linear(input_dim, out_dim, bias=True)\n",
    "\n",
    "#lin.weight.data = component_scale_matrix(x=0.5, y=0.5)[:2, :2]\n",
    "\n",
    "x = torch.randn(out_dim, input_dim)\n",
    "x_shifted = x + 0.5\n",
    "\n",
    "x_2 = torch.randn(5, input_dim)\n",
    "x_shifted2 = x_2 + 0.5\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_transf = lin(x)\n",
    "    x_transf2 = lin(x_2)\n",
    "\n",
    "X = np.hstack((x_shifted.numpy(), np.ones((x_shifted.shape[0], 1))))\n",
    "theta = np.linalg.inv(X.T @ X) @ (X.T @ x_transf.numpy())\n",
    "\n",
    "lin_new = torch.nn.Linear(input_dim, out_dim, bias=True)\n",
    "lin_new.weight.data = torch.tensor(theta[:-1, :].T, dtype=torch.float32)\n",
    "lin_new.bias.data = torch.tensor(theta[-1, :], dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_shifted_transf = lin_new(x_shifted)\n",
    "    x_shifted_transf2 = lin_new(x_shifted2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.close(\"all\")\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "p1 = ax.plot(x_2[:, 0], x_2[:, 1], 0, \"o\", label=\"Original\")\n",
    "p1 = ax.plot(x_shifted2[:, 0], x_shifted2[:, 1], 0, \"o\", label=\"Shifted\")\n",
    "p2 = ax.plot(x_transf2[:, 0], x_transf2[:, 1],  x_transf2[:, 2], \"o\", label=\"Transformed\")\n",
    "p2 = ax.plot(x_shifted_transf2[:, 0], x_shifted_transf2[:, 1], x_shifted_transf2[:, 2], \".\", label=\"Shifted Transformed\")\n",
    "\n",
    "plt.legend()\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((X, np.ones((X.shape[0], 1))))\n",
    "theta = np.linalg.solve(X.T @ X, X.T @ Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_transformation_matrix(x=0.5, y=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.run.transforms2d import component_transformation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(res_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(res_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.util.temporary_property import TemporaryProperty\n",
    "\n",
    "\n",
    "with TemporaryProperty(runner.dataloader, mode=\"sample\", return_prior=False):\n",
    "    fig = runner.dataloader[0][\"raw_sample\"].plot_weak_labels()\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = runner.dataloader.__dataset__\n",
    "import awesome.run.functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "index = 0\n",
    "\n",
    "size = 10\n",
    "cols = 4\n",
    "fig, ax = plt.subplots(1, cols, figsize=(size * cols, size))\n",
    "\n",
    "sample = dataloader[index]\n",
    "\n",
    "sample.plot(ax=ax[0], labels=sample.ground_truth_object_ids)\n",
    "ax[0].set_title(\"Ground truth\")\n",
    "\n",
    "sample.plot_weak_labels(ax=ax[1])\n",
    "ax[1].set_title(\"Weak labels\")\n",
    "\n",
    "sample.plot_selected_weak_labels(ax=ax[2])\n",
    "ax[2].set_title(\"Selected Weak labels\")\n",
    "\n",
    "sample.plot_selected(ax=ax[3], labels=[\"Foreground\", \"Background\"])\n",
    "ax[3].set_title(\"Selected ground truth\")\n",
    "\n",
    "\n",
    "mapping = sample._get_gt_object_id_weak_label_mapping()\n",
    "display(mapping)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"Stop here\"\n",
    "# Code for extracting the trajectories into the dataset folder\n",
    "tracks_path = \"data/local_datasets/FBMS-59/tracks\"\n",
    "dataset_dirs = \"data/local_datasets/FBMS-59/test/\"\n",
    "\n",
    "import shutil\n",
    "\n",
    "for folder in os.listdir(tracks_path):\n",
    "    inner_path = \"MulticutResults/pfldof0.5000004\"\n",
    "    complete_track_path = os.path.join(tracks_path, folder, inner_path)\n",
    "    tracks_file = list(os.listdir(complete_track_path))[0]\n",
    "    tracks_file_path = os.path.join(complete_track_path, tracks_file)\n",
    "\n",
    "    target_path = os.path.join(dataset_dirs, folder, \"tracks\", \"multicut\")\n",
    "    os.makedirs(target_path, exist_ok=True)\n",
    "    target_file_path = os.path.join(target_path, tracks_file)\n",
    "    shutil.copy(tracks_file_path, target_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awesome-dC4phDSK-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

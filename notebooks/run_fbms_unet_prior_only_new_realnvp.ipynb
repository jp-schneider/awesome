{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\projects\\AWESOME\\awesome\\agent\\torch_agent.py:20: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from awesome.run.awesome_config import AwesomeConfig\n",
    "from awesome.run.awesome_runner import AwesomeRunner\n",
    "from awesome.util.reflection import class_name\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from awesome.model.convex_diffeomorphism_net import ConvexDiffeomorphismNet\n",
    "from awesome.util.path_tools import get_project_root_path\n",
    "from awesome.util.logging import basic_config\n",
    "from awesome.dataset.awesome_dataset import AwesomeDataset\n",
    "from awesome.dataset.fbms_sequence_dataset import FBMSSequenceDataset\n",
    "from awesome.measures.fbms_joint_loss import FBMSJointLoss\n",
    "from awesome.model.unet import UNet\n",
    "from awesome.measures.weighted_loss import WeightedLoss\n",
    "from awesome.measures.se import SE\n",
    "from awesome.measures.unaries_conversion_loss import UnariesConversionLoss\n",
    "from typing import Literal\n",
    "\n",
    "basic_config()\n",
    "\n",
    "os.chdir(get_project_root_path()) # Beeing in the root directory of the project is important for the relative paths to work consistently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\projects\\AWESOME\\awesome\\dataset\\fbms_sequence_dataset.py:732: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3575.)\n",
      "  bg_flip_coords = flip_probability[torch.argwhere(bg_flip_mask).squeeze(), :2].int().T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./config/UNET+bear01+edge+diffeo+only_prior+realnvp.yaml'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from awesome.model.zoo import Zoo\n",
    "from awesome.model.net_factory import real_nvp_path_connected_net\n",
    "\n",
    "xytype = \"edge\"\n",
    "dataset_kind = \"train\"\n",
    "dataset = \"bear01\"\n",
    "all_frames = True\n",
    "subset = None # 0 #slice(0, 5)\n",
    "segmentation_model_switch: Literal[\"original\", \"retrain\", \"retrain_xy\"] = \"original\"\n",
    "\n",
    "\n",
    "segmentation_model_state_dict_path = None\n",
    "if segmentation_model_switch == \"original\":\n",
    "    segmentation_model_state_dict_path = f\"./data/checkpoints/labels_with_uncertainty_flownet2_based/model_{dataset}_unet.pth\"\n",
    "elif segmentation_model_switch == \"retrain\":\n",
    "    segmentation_model_state_dict_path = f\"./data/checkpoints/refit_unet_uncertainty/23_11_13/model_{dataset}_unet.pth\"\n",
    "elif segmentation_model_switch == \"retrain_xy\":\n",
    "    segmentation_model_state_dict_path = f\"./data/checkpoints/refit_spatial_unet_uncertainty/23_11_13/model_{dataset}_unet.pth\"\n",
    "else:\n",
    "    raise ValueError(f\"Unknown segmentation_model_switch: {segmentation_model_switch}\")\n",
    "image_channel_format = \"bgr\" if segmentation_model_switch == \"original\" else \"rgb\"\n",
    "input_channels = 4 if xytype == \"edge\" else 6\n",
    "prior_criterion = UnariesConversionLoss(SE(reduction=\"mean\"))\n",
    "\n",
    "data_path = f\"./data/local_datasets/FBMS-59/{dataset_kind}/{dataset}\"\n",
    "\n",
    "real_dataset = FBMSSequenceDataset(\n",
    "                    dataset_path=data_path,\n",
    "                    weak_labels_dir = \"weak_labels/labels_with_uncertainty_flownet2_based\",\n",
    "                    processed_weak_labels_dir = \"weak_labels/labels_with_uncertainty_flownet2_based/processed\",\n",
    "                    confidence_dir= \"weak_labels/labels_with_uncertainty_flownet2_based/\",\n",
    "                    do_weak_label_preprocessing=True,\n",
    "                    do_uncertainty_label_flip=True,\n",
    "                    test_weak_label_integrity=False,\n",
    "                    all_frames=True,\n",
    "                )\n",
    "data_path = f\"./data/local_datasets/FBMS-59/{dataset_kind}/{dataset}\"\n",
    "cfg = AwesomeConfig(\n",
    "        name_experiment=f\"UNET+{dataset}+{xytype}+diffeo+only_prior+realnvp\",\n",
    "        dataset_type=class_name(AwesomeDataset),\n",
    "        dataset_args={\n",
    "            \"dataset\": real_dataset,\n",
    "            \"xytype\": xytype,\n",
    "            \"feature_dir\": f\"{data_path}/Feat\",\n",
    "            \"dimension\": \"3d\", # 2d for fcnet\n",
    "            \"mode\": \"model_input\",\n",
    "            \"model_input_requires_grad\": False,\n",
    "            \"batch_size\": 1,\n",
    "            \"split_ratio\": 1,\n",
    "            \"shuffle_in_dataloader\": False,\n",
    "            \"image_channel_format\": image_channel_format,\n",
    "            \"do_image_blurring\": True,\n",
    "            \"subset\": subset\n",
    "        },\n",
    "        segmentation_model_type=class_name(UNet),\n",
    "        segmentation_model_args={\n",
    "            'in_chn': input_channels,\n",
    "        },\n",
    "        segmentation_training_mode='multi',\n",
    "        segmentation_model_state_dict_path=segmentation_model_state_dict_path, # Path to the pretrained model\n",
    "        use_segmentation_output_inversion=True,\n",
    "        use_prior_model=True,\n",
    "        prior_model_args=dict(\n",
    "            channels=2,\n",
    "            hidden_units=32,\n",
    "            flow_n_flows=12,\n",
    "            flow_output_fn=\"tanh\",\n",
    "            norm=\"minmax\",\n",
    "            convex_net_hidden_units=130,\n",
    "            convex_net_hidden_layers=2,\n",
    "        ),\n",
    "        prior_model_type=class_name(real_nvp_path_connected_net),\n",
    "        loss_type=class_name(FBMSJointLoss),\n",
    "        loss_args={\n",
    "            \"criterion\": WeightedLoss(torch.nn.BCELoss(), mode=\"sssdms\", noneclass=2),\n",
    "            \"alpha\": 1,\n",
    "            \"beta\": 1,\n",
    "        },\n",
    "        use_extra_penalty_hook=False, # Panalty hook for the panalty term that models output should match\n",
    "        #extra_penalty_after_n_epochs=1,\n",
    "        #use_reduce_lr_in_extra_penalty_hook=False,\n",
    "        use_lr_on_plateau_scheduler=False,\n",
    "        use_binary_classification=True, \n",
    "        num_epochs=100,\n",
    "        device=\"cuda\",\n",
    "        dtype=str(torch.float32),\n",
    "        runs_path=\"./runs/fbms_local/unet/comparison_path_nets\",\n",
    "        optimizer_args={\n",
    "            \"lr\": 0.003,\n",
    "            \"betas\": (0.9, 0.999),\n",
    "            \"eps\": 1e-08,\n",
    "            \"amsgrad\": False\n",
    "        },\n",
    "        use_progress_bar=True,\n",
    "        plot_indices_during_training_nth_epoch=5,\n",
    "        plot_indices_during_training=real_dataset.get_ground_truth_indices(),\n",
    "        save_images_after_pretraining=True,\n",
    "        include_unaries_when_saving=True,\n",
    "        agent_args=dict(\n",
    "             do_pretraining=True,\n",
    "             pretrain_only=True, \n",
    "             force_pretrain=True,\n",
    "             pretrain_state_path=f\"./data/checkpoints/pretrain_states/model_{dataset}_unet_spatial_{all_frames}_{subset}_realnvp.pth\",\n",
    "             pretrain_args=dict(\n",
    "                 use_pretrain_checkpoints=True,\n",
    "                 do_pretrain_checkpoints=True,\n",
    "                 pretrain_checkpoint_dir=f\"./data/checkpoints/pretrain_states/model_{dataset}_unet_spatial_{all_frames}_{subset}_realnvp\",\n",
    "                 lr=0.001,\n",
    "                 use_logger=True,\n",
    "                 use_step_logger=True,\n",
    "                 num_epochs=4000,\n",
    "                 proper_prior_fit_retrys=1,\n",
    "                 reuse_state_epochs=400,\n",
    "                 # Prefit flow net identity => Flow will be identity(-like) at the beginning\n",
    "                 prefit_flow_net_identity=True,\n",
    "                 prefit_flow_net_identity_lr=1e-2,\n",
    "                 prefit_flow_net_identity_weight_decay=1e-5,\n",
    "                 prefit_flow_net_identity_num_epochs=100,\n",
    "                 # Prefit convex net, to start with a convex thing\n",
    "                 prefit_convex_net=True,\n",
    "                 prefit_convex_net_lr=1e-3,\n",
    "                 prefit_convex_net_weight_decay=0,\n",
    "                 prefit_convex_net_num_epochs=200,\n",
    "                 zoo=Zoo()\n",
    "             )\n",
    "        ),\n",
    "        #output_folder=\"./runs/fbms_local/unet/TestUnet/\",\n",
    "    )\n",
    "cfg.save_to_file(f\"./config/{cfg.name_experiment}.yaml\", override=True, no_uuid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Schneider\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\awesome-dC4phDSK-py3.9\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "2024-01-11:12:40:17.526 INFO     [tensorboard.py:55] Tensorboard logger created at: runs\\fbms_local\\unet\\comparison_path_nets\\UNET+bear01+edge+diffeo+only_prior+realnvp_24_01_11_12_40_17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./runs/fbms_local/unet/comparison_path_nets\\\\UNET+bear01+edge+diffeo+only_prior+realnvp_24_01_11_12_40_17\\\\init_cfg_awesome_config.yaml'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner = AwesomeRunner(cfg)\n",
    "runner.build()\n",
    "runner.store_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.agent._get_model().prior_module.reset_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runner.config.num_epochs = 2000\n",
    "runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.run.functions import get_result, split_model_result, plot_image_scribbles\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from awesome.run.functions import get_mpl_figure, plot_mask, prepare_input_eval\n",
    "from awesome.util.matplotlib import saveable\n",
    "import normflows as nf\n",
    "from typing import Optional, Tuple\n",
    "from awesome.model.path_connected_net import PathConnectedNet, minmax\n",
    "from matplotlib.axes import Axes\n",
    "\n",
    "def coordinate_grid(image_shape):\n",
    "    x = torch.arange(image_shape[1]).float()\n",
    "    y = torch.arange(image_shape[0]).float()\n",
    "    yy, xx = torch.meshgrid(y, x)\n",
    "    return torch.stack([yy, xx])\n",
    "\n",
    "def create_circle(image_shape: Tuple[int, int], radius: float, center: Tuple[float, float]):\n",
    "    grid = coordinate_grid(image_shape)\n",
    "    yy, xx = grid\n",
    "    circle = (yy - center[0])**2 + (xx - center[1])**2 <= radius**2\n",
    "    return circle.float()[None, ...]\n",
    "\n",
    "def subsample_mask(x,\n",
    "                   subsample: int = 25):\n",
    "    image_shape = x.shape[-2:]\n",
    "\n",
    "    ones_grid = torch.ones(x[0].shape)\n",
    "    subsampled_grid = torch.zeros(x[0].shape)\n",
    "    coords = (torch.argwhere(ones_grid) % subsample) == 0\n",
    "    coords_mask = coords.all(dim=-1).reshape((image_shape))\n",
    "    subsampled_grid[coords_mask] = 1\n",
    "    return subsampled_grid.bool()\n",
    "\n",
    "@saveable()\n",
    "def plot_output(img, \n",
    "                output, \n",
    "                target, \n",
    "                grid: torch.Tensor, \n",
    "                subsample:int = 25, \n",
    "                **kwargs):\n",
    "    image_shape = grid.shape[-2:]\n",
    "    def denorm_grid(grid):\n",
    "        image_shape = grid.shape[-2:]\n",
    "        grid_y = minmax(grid[0], grid[0].min(), grid[0].max(), 0, image_shape[0])\n",
    "        grid_x = minmax(grid[1], grid[1].min(), grid[1].max(), 0, image_shape[1])\n",
    "        grid_dnorm = torch.cat([grid_y[None, ...], grid_x[None, ...]], dim=0).detach().cpu().numpy()\n",
    "        return grid_dnorm\n",
    "\n",
    "    fig = plot_match(img, output, target, size=5, tight=True, subsample=subsample)\n",
    "    ax = fig.axes[0]\n",
    "    dnorm_grid_pt = denorm_grid(grid)\n",
    "\n",
    "    dnorm_grid_pt = torch.clamp(torch.tensor(dnorm_grid_pt), min=torch.tensor([[[0]], [[0]]]), max=torch.tensor([[[image_shape[0] - 1]], [[image_shape[1] - 1]]])).numpy()\n",
    "    \n",
    "    msk = subsample_mask(dnorm_grid_pt, subsample=subsample)\n",
    "\n",
    "    return plot_grid(dnorm_grid_pt, msk, ax, 'g', origin=\"lower\")\n",
    "    \n",
    "@saveable()\n",
    "def plot_grid(grid: torch.Tensor, \n",
    "              mask: torch.Tensor, \n",
    "              ax: Optional[Axes] = None, \n",
    "              tight: Optional[bool] = False,\n",
    "              size: Optional[float] = 5,\n",
    "              color: str = \"b\",\n",
    "              dense: bool = True,\n",
    "              origin: Literal['lower', 'upper'] = \"upper\"\n",
    "              ):\n",
    "    if ax is None:\n",
    "        fig, ax = get_mpl_figure(1, 1, tight=tight, size=size, ratio_or_img=grid)\n",
    "    else:\n",
    "        fig = ax.figure\n",
    "\n",
    "    dots = torch.argwhere(mask)\n",
    "\n",
    "    col_idx = torch.unique(dots[:, 0])\n",
    "    row_idx = torch.unique(dots[:, 1])\n",
    "\n",
    "    cols = grid[:, col_idx]\n",
    "    rows = grid[:, :, row_idx]\n",
    "\n",
    "    for idx in range(cols.shape[1]):\n",
    "        col = cols[:, idx]\n",
    "        if dense:\n",
    "            x = col[1]\n",
    "            y = col[0]\n",
    "        else:\n",
    "            x = col[1, row_idx]\n",
    "            y = col[0, row_idx]\n",
    "        ax.plot(x, y, color=color)\n",
    "\n",
    "    for idx in range(rows.shape[2]):\n",
    "        row = rows[:, :, idx]\n",
    "        if dense:\n",
    "            x = row[1]\n",
    "            y = row[0]\n",
    "        else:\n",
    "            x = row[1, col_idx]\n",
    "            y = row[0, col_idx]\n",
    "        ax.plot(x, y, color=color)\n",
    "\n",
    "    if origin == \"upper\":\n",
    "        ax.invert_yaxis()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_match(img, \n",
    "               output, \n",
    "               target, \n",
    "               subsample:int = 25, \n",
    "               grid: torch.Tensor = None,\n",
    "               **kwargs):\n",
    "\n",
    "    image_shape = img.shape[-2:]\n",
    "    if grid is None:\n",
    "        grid = coordinate_grid(image_shape)\n",
    "\n",
    "    subsampled_grid = torch.zeros_like(grid[0])\n",
    "\n",
    "    coords_mask = subsample_mask(grid, subsample=subsample)\n",
    "    subsampled_grid[coords_mask] = 1\n",
    "\n",
    "    add = []\n",
    "\n",
    "    if target is not None:\n",
    "        add.append(target)\n",
    "    if output is not None:\n",
    "        add.append(output)\n",
    "    \n",
    "    add.append(subsampled_grid.float()[None, ...])\n",
    "\n",
    "    stack_plot = torch.cat(add, dim=0)\n",
    "\n",
    "    fig = plot_mask(img, stack_plot, **kwargs)\n",
    "    return fig\n",
    "\n",
    "index = 0\n",
    "\n",
    "model = runner.agent._get_model()\n",
    "dataloader = runner.agent.training_dataset\n",
    "model_gets_targets = runner.agent.model_gets_targets\n",
    "p = os.path.join(runner.agent.agent_folder, \"pretrain_priors\")\n",
    "os.makedirs(p, exist_ok=True)\n",
    "\n",
    "#indices = [0, 19] #len(dataloader)\n",
    "for i in range(len(dataloader)):\n",
    "    res, ground_truth, img, fg, bg = get_result(model, dataloader, i, model_gets_targets=model_gets_targets)\n",
    "    res = split_model_result(res, model, dataloader, img)\n",
    "    res_prior = res.get(\"prior\", None)\n",
    "    res_pred = res[\"segmentation\"]\n",
    "    boxes = res.get(\"boxes\", None)\n",
    "    labels = res.get(\"labels\", None)\n",
    "    \n",
    "    iterations = 2000\n",
    "    fig = plot_image_scribbles(image=img,\n",
    "                        inference_result=res_pred,\n",
    "                        foreground_mask=fg,\n",
    "                        background_mask=bg,\n",
    "                        prior_result=res_prior,\n",
    "                        save=True,\n",
    "                        path=os.path.join(p, f\"prior_{i}_{iterations}.png\"),\n",
    "                        size=10,\n",
    "                        title=f\"Prior Epoch: {iterations}\", open=True)\n",
    "    #display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model.to(torch.device(\"cpu\"))\n",
    "        image, ground_truth, _input, targets, fg, bg, prior_state = prepare_input_eval(dataloader, model, index)\n",
    "        grid = model.prior_module.get_deformation(_input[2][None, ...])[0]\n",
    "\n",
    "    fig = plot_output(img, 1 - res_prior, 1 - res_pred, grid=grid, size=30, subsample=10,\n",
    "                    save=True,\n",
    "                        path=os.path.join(p, f\"deform_grid_{i}_{iterations}.png\"),\n",
    "                    )\n",
    "\n",
    "    #display(fig)\n",
    "    plt.close(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.run.functions import get_mpl_figure, plot_mask, prepare_input_eval\n",
    "from awesome.util.matplotlib import saveable\n",
    "import normflows as nf\n",
    "from typing import Optional, Tuple\n",
    "from awesome.model.path_connected_net import PathConnectedNet, minmax\n",
    "from matplotlib.axes import Axes\n",
    "\n",
    "def coordinate_grid(image_shape):\n",
    "    x = torch.arange(image_shape[1]).float()\n",
    "    y = torch.arange(image_shape[0]).float()\n",
    "    yy, xx = torch.meshgrid(y, x)\n",
    "    return torch.stack([yy, xx])\n",
    "\n",
    "def create_circle(image_shape: Tuple[int, int], radius: float, center: Tuple[float, float]):\n",
    "    grid = coordinate_grid(image_shape)\n",
    "    yy, xx = grid\n",
    "    circle = (yy - center[0])**2 + (xx - center[1])**2 <= radius**2\n",
    "    return circle.float()[None, ...]\n",
    "\n",
    "def subsample_mask(x,\n",
    "                   subsample: int = 25):\n",
    "    image_shape = x.shape[-2:]\n",
    "\n",
    "    ones_grid = torch.ones(x[0].shape)\n",
    "    subsampled_grid = torch.zeros(x[0].shape)\n",
    "    coords = (torch.argwhere(ones_grid) % subsample) == 0\n",
    "    coords_mask = coords.all(dim=-1).reshape((image_shape))\n",
    "    subsampled_grid[coords_mask] = 1\n",
    "    return subsampled_grid.bool()\n",
    "\n",
    "@saveable()\n",
    "def plot_output(img, \n",
    "                output, \n",
    "                target, \n",
    "                grid: torch.Tensor, \n",
    "                subsample:int = 25, \n",
    "                **kwargs):\n",
    "    image_shape = grid.shape[-2:]\n",
    "    def denorm_grid(grid):\n",
    "        image_shape = grid.shape[-2:]\n",
    "        grid_y = minmax(grid[0], grid[0].min(), grid[0].max(), 0, image_shape[0])\n",
    "        grid_x = minmax(grid[1], grid[1].min(), grid[1].max(), 0, image_shape[1])\n",
    "        grid_dnorm = torch.cat([grid_y[None, ...], grid_x[None, ...]], dim=0).detach().cpu().numpy()\n",
    "        return grid_dnorm\n",
    "\n",
    "    fig = plot_match(img, output, target, size=5, tight=True, subsample=subsample)\n",
    "    ax = fig.axes[0]\n",
    "    dnorm_grid_pt = denorm_grid(grid)\n",
    "\n",
    "    dnorm_grid_pt = torch.clamp(torch.tensor(dnorm_grid_pt), min=torch.tensor([[[0]], [[0]]]), max=torch.tensor([[[image_shape[0] - 1]], [[image_shape[1] - 1]]])).numpy()\n",
    "    \n",
    "    msk = subsample_mask(dnorm_grid_pt, subsample=subsample)\n",
    "\n",
    "    return plot_grid(dnorm_grid_pt, msk, ax, 'g', origin=\"lower\")\n",
    "    \n",
    "@saveable()\n",
    "def plot_grid(grid: torch.Tensor, \n",
    "              mask: torch.Tensor, \n",
    "              ax: Optional[Axes] = None, \n",
    "              tight: Optional[bool] = False,\n",
    "              size: Optional[float] = 5,\n",
    "              color: str = \"b\",\n",
    "              dense: bool = True,\n",
    "              origin: Literal['lower', 'upper'] = \"upper\"\n",
    "              ):\n",
    "    if ax is None:\n",
    "        fig, ax = get_mpl_figure(1, 1, tight=tight, size=size, ratio_or_img=grid)\n",
    "    else:\n",
    "        fig = ax.figure\n",
    "\n",
    "    dots = torch.argwhere(mask)\n",
    "\n",
    "    col_idx = torch.unique(dots[:, 0])\n",
    "    row_idx = torch.unique(dots[:, 1])\n",
    "\n",
    "    cols = grid[:, col_idx]\n",
    "    rows = grid[:, :, row_idx]\n",
    "\n",
    "    for idx in range(cols.shape[1]):\n",
    "        col = cols[:, idx]\n",
    "        if dense:\n",
    "            x = col[1]\n",
    "            y = col[0]\n",
    "        else:\n",
    "            x = col[1, row_idx]\n",
    "            y = col[0, row_idx]\n",
    "        ax.plot(x, y, color=color)\n",
    "\n",
    "    for idx in range(rows.shape[2]):\n",
    "        row = rows[:, :, idx]\n",
    "        if dense:\n",
    "            x = row[1]\n",
    "            y = row[0]\n",
    "        else:\n",
    "            x = row[1, col_idx]\n",
    "            y = row[0, col_idx]\n",
    "        ax.plot(x, y, color=color)\n",
    "\n",
    "    if origin == \"upper\":\n",
    "        ax.invert_yaxis()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_match(img, \n",
    "               output, \n",
    "               target, \n",
    "               subsample:int = 25, \n",
    "               grid: torch.Tensor = None,\n",
    "               **kwargs):\n",
    "\n",
    "    image_shape = img.shape[-2:]\n",
    "    if grid is None:\n",
    "        grid = coordinate_grid(image_shape)\n",
    "\n",
    "    subsampled_grid = torch.zeros_like(grid[0])\n",
    "\n",
    "    coords_mask = subsample_mask(grid, subsample=subsample)\n",
    "    subsampled_grid[coords_mask] = 1\n",
    "\n",
    "    add = []\n",
    "\n",
    "    if target is not None:\n",
    "        add.append(target)\n",
    "    if output is not None:\n",
    "        add.append(output)\n",
    "    \n",
    "    add.append(subsampled_grid.float()[None, ...])\n",
    "\n",
    "    stack_plot = torch.cat(add, dim=0)\n",
    "\n",
    "    fig = plot_mask(img, stack_plot, **kwargs)\n",
    "    return fig\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awesome-dC4phDSK-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

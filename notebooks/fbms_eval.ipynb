{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from awesome.run.awesome_config import AwesomeConfig\n",
    "from awesome.run.awesome_runner import AwesomeRunner\n",
    "from awesome.util.reflection import class_name\n",
    "from awesome.analytics.result_model import ResultModel\n",
    "from awesome.util.path_tools import get_project_root_path, get_package_root_path\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "from awesome.util.format import latex_postprocessor\n",
    "import pandas as pd\n",
    "from typing import Any, Dict\n",
    "os.chdir(get_project_root_path()) # Beeing in the root directory of the project is important for the relative paths to work consistently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.analytics.result_comparison import ResultComparison\n",
    "\n",
    "paths = [\n",
    "         \"./runs/fbms_local/eval/unet/joint_realnvp/2024-01-11\"\n",
    "         ]\n",
    "models = []\n",
    "\n",
    "for path in paths:\n",
    "    for folder in os.listdir(path):\n",
    "        if folder.startswith(\"old\") or folder.startswith(\"log\"):\n",
    "            continue\n",
    "        model = ResultModel.from_path(os.path.join(path, folder))\n",
    "        models.append(model)\n",
    "\n",
    "import re\n",
    "p = r\"#?(?P<cfg_num>\\d+)?_?(?P<net>[A-z0-9]+)_?(?P<feat>(\\+\\w+)*)\\_(?P<date>\\d{2}_\\d{2}_\\d{2})\\_(?P<time>\\d{2}_\\d{2}_\\d{2})\"\n",
    "pattern = re.compile(p)\n",
    "\n",
    "for model in models:\n",
    "    match = pattern.match(model.name)\n",
    "    model_name = None\n",
    "    feat = []\n",
    "    if match:\n",
    "        model_name = match.group('net').strip(\"_\")\n",
    "        features = match.group('feat')\n",
    "        if features is not None and features != \"\":\n",
    "            feat = features.strip(\"+\").split(\"+\")\n",
    "            if not any([\"seed\" in x for x in feat]):\n",
    "                feat.append(\"seed42\")\n",
    "    else:\n",
    "        print('No match for', model.name)\n",
    "    model_name = model_name.replace(\"NET\", \"Net\")\n",
    "    model.display_name = model_name + \" \" + \" \".join(feat)\n",
    "    model.features = list(feat)\n",
    "    model.config.result_directory = \"final_mask\"\n",
    "    model.save_config()\n",
    "\n",
    "\n",
    "# Resort the models by name to get a meaningful table order\n",
    "\n",
    "_order = []\n",
    "\n",
    "models = sorted(models, key=lambda m: _order.index(m.name) if m.name in _order else 0)\n",
    "\n",
    "comparison = ResultComparison(models)\n",
    "comparison.assign_numbers(force=True)\n",
    "\n",
    "os.environ['PLOT_OUTPUT_DIR'] = comparison.output_folder\n",
    "\n",
    "save_args = dict(transparent=False, save=True, dpi=300, ext=[\"png\", \"pdf\"])\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "metrics = [\n",
    "    \"eval/epoch/MeanForegroundBinaryMIOU\" ,\n",
    "    \"eval/epoch/MeanPriorForegroundBinaryMIOU\",\n",
    "    \"eval/epoch/MeanPixelAccuracy\",\n",
    "    \"eval/epoch/MeanPriorPixelAccuracy\",\n",
    "    \"eval/epoch/MeanCRFForegroundBinaryMIOU\",\n",
    "    \"eval/epoch/MeanCRFPixelAccuracy\",\n",
    "]\n",
    "\n",
    "col_mapping = {\n",
    "    \"eval/epoch/MeanForegroundBinaryMIOU\": \"IoU\",\n",
    "    \"eval/epoch/MeanCRFForegroundBinaryMIOU\": \"CRF IoU\",\n",
    "    \"eval/epoch/MeanPixelAccuracy\": \"Acc.\",\n",
    "    \"eval/epoch/MeanCRFPixelAccuracy\": \"CRF Acc.\",\n",
    "    \"eval/epoch/MeanPriorPixelAccuracy\" : \"Prior Acc.\",\n",
    "    \"eval/epoch/MeanPriorForegroundBinaryMIOU\": \"Prior IoU\" \n",
    "}\n",
    "\n",
    "index_mapping = {\n",
    "    0: \"Baseline\",\n",
    "    15: \"Joint\"\n",
    "}\n",
    "\n",
    "new_colmapping = {}\n",
    "for k, v in dict(col_mapping).items():\n",
    "    for k_i, v_i in dict(index_mapping).items():\n",
    "        new_colmapping[k + \"_\" + str(k_i)] = v + \" \" + v_i\n",
    "\n",
    "col_mapping = new_colmapping\n",
    "\n",
    "def extract_features(model: ResultModel) -> Dict[str, Any]:\n",
    "    res = dict()\n",
    "    \n",
    "    res['joint'] = \"joint\" in model.features\n",
    "    \n",
    "    res['model_name'] = model.config.name.split(\" \")[0]\n",
    "    model_features = list(model.features)\n",
    "    \n",
    "    if res['joint']:\n",
    "        model_features.remove(\"joint\")\n",
    "    \n",
    "    seed = next((x for x in model_features if \"seed\" in x), None)\n",
    "    if seed is None:\n",
    "        seed = model.run_config.seed\n",
    "        res['seed'] = seed\n",
    "    else:\n",
    "        model_features.remove(seed)\n",
    "        res['seed'] = int(seed.replace(\"seed\", \"\"))\n",
    "\n",
    "    if \"REFIT\" in model_features:\n",
    "        model_features.remove(\"REFIT\")\n",
    "        res['prior'] = \"refit\"\n",
    "\n",
    "    if \"original\" in model_features:\n",
    "        model_features.remove(\"original\")\n",
    "        res['prior'] = \"original\"\n",
    "    if \"retrain\" in model_features:\n",
    "        model_features.remove(\"retrain\")\n",
    "        res['prior'] = \"refit\"\n",
    "    if \"retrain_xy\" in model_features:\n",
    "        model_features.remove(\"retrain_xy\")\n",
    "        res['prior'] = \"refit\"\n",
    "\n",
    "    elif \"convex\" in model_features:\n",
    "        model_features.remove(\"convex\")\n",
    "        res['prior'] = \"convex\"\n",
    "    elif \"diffeo\" in model_features:\n",
    "        model_features.remove(\"diffeo\")\n",
    "        res['prior'] = \"diffeo\"\n",
    "    else:\n",
    "        res['prior'] = \"none\"\n",
    "\n",
    "    if \"only_prior\" in model_features:\n",
    "        model_features.remove(\"only_prior\")\n",
    "        res['prior'] = res['prior'] + \"+only_prior\"\n",
    "\n",
    "    if \"all_frames\" in model_features:\n",
    "        model_features.remove(\"all_frames\")\n",
    "        res['prior'] = res['prior'] + \"+all_frames\"\n",
    "    if \"deeper\" in model_features:\n",
    "        model_features.remove(\"deeper\")\n",
    "        res['prior'] = res['prior'] + \"+deeper\"\n",
    "\n",
    "    dataset_name = model_features.pop(0)\n",
    "    res['dataset_name'] = dataset_name\n",
    "\n",
    "    assert len(model_features) == 1, f\"Multiple features {model_features} in model {model.output_path}\"\n",
    "    res['feature_type'] = model_features[0]\n",
    "\n",
    "    return res\n",
    "\n",
    "df = comparison.metric_table(metrics, \n",
    "                             ref=\"all\", \n",
    "                             mode=\"max\",\n",
    "                        formatting=False)\n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "def extract_ft_row(row: pd.Series) -> Tuple[str, bool, str, int]:\n",
    "    name = row['index']\n",
    "    model = [m for m in models if m.name == name][0]\n",
    "    res = extract_features(model)\n",
    "    return (res['model_name'], res['joint'], res['feature_type'], res['seed'], res['dataset_name'], res['prior'])\n",
    "\n",
    "df[['model_name', 'joint', 'feature_type', 'seed', 'dataset_name', \"prior\"]] = df.apply(extract_ft_row, axis=1, result_type=\"expand\")\n",
    "\n",
    "df = df[['model_name', 'dataset_name'] + list(col_mapping.keys()) + ['joint', 'feature_type', 'seed', \"prior\"]]\n",
    "\n",
    "grouped = df.groupby(['model_name', 'joint', 'feature_type'])\n",
    "\n",
    "display_df = df.rename(columns=col_mapping)\n",
    "display(display_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(display_df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(display_df.describe().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_ds = set(display_df['dataset_name'].values.tolist())\n",
    "\n",
    "should = set([\n",
    "   'bear01',\n",
    "   'bear02',\n",
    "   'cars2',\n",
    "   'cars3',\n",
    "   'cars6',\n",
    "   'cars7',\n",
    "   'cars8',\n",
    "   'cats04',\n",
    "   'cats05',\n",
    "   'horses01',\n",
    "   'horses03',\n",
    "   'marple1',\n",
    "   'marple10',\n",
    "   'marple11',\n",
    "   'marple5',\n",
    "   'meerkats01',\n",
    "   'people04',\n",
    "   'rabbits01',\n",
    "   ])\n",
    "should - ev_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_df = df.rename(columns=col_mapping)\n",
    "\n",
    "diffeo_df = renamed_df#[(renamed_df['prior'] == 'diffeo') | (renamed_df['prior'] == 'diffeo+only_prior')]\n",
    "order = ['model_name', 'joint', 'feature_type', 'dataset_name', 'IoU', 'Prior IoU', 'Acc.', 'Prior Acc.', 'prior']\n",
    "diffeo_df[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diffeo_df[order].to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display_df = diffeo_df[order]\n",
    "\n",
    "display_df.loc[:, \"prior\"] = display_df[\"prior\"].values + display_df[\"joint\"].apply(lambda x: \"+joint\" if x else \"\")\n",
    "display_df.loc[:, \"prior\"] = display_df[\"prior\"].str.replace(\"diffeo+\", \"\")\n",
    "\n",
    "display_df.loc[:, \"prior\"] = display_df[\"prior\"].str.replace(\"only_prior+all_frames+deeper\", \"Prior fit only (all+deeper)\")\n",
    "\n",
    "display_df.loc[display_df['feature_type'] == \"edge\", \"prior\"] = display_df[display_df['feature_type'] == \"edge\"][\"prior\"].str.replace(\"refit\", \"Refit\")\n",
    "display_df.loc[display_df['feature_type'] == \"edgexy\", \"prior\"] = display_df[display_df['feature_type'] == \"edgexy\"][\"prior\"].str.replace(\"refit\", \"Refit XY\")\n",
    "\n",
    "\n",
    "\n",
    "display_df.loc[:, \"prior\"] = display_df[\"prior\"].str.replace(\"only_prior+all_frames\", \"Prior fit only (all)\")\n",
    "display_df.loc[:, \"prior\"] = display_df[\"prior\"].str.replace(\"only_prior+deeper\", \"Prior fit only (GT+deeper)\")\n",
    "display_df.loc[:, \"prior\"] = display_df[\"prior\"].str.replace(\"only_prior\", \"Prior fit only (GT)\")\n",
    "display_df.loc[:, \"prior\"] = display_df[\"prior\"].str.replace(\"deeper+joint\", \"Joint Training (deeper)\")\n",
    "display_df.loc[:, \"prior\"] = display_df[\"prior\"].str.replace(\"joint\", \"Joint Training\")\n",
    "\n",
    "prior_categories = [\n",
    "                    \"Refit\",\n",
    "                    \"Refit XY\",\n",
    "                    \"Prior fit only (GT)\", \n",
    "                    \"Prior fit only (GT+deeper)\", \n",
    "                    \"Prior fit only (all)\", \n",
    "                    \"Prior fit only (all+deeper)\", \n",
    "                    \"Joint Training\",\n",
    "                    \"Joint Training (deeper)\",\n",
    "                    ]\n",
    "\n",
    "\n",
    "display_df = display_df.drop(columns=[\"joint\", \"feature_type\", \"model_name\"])\n",
    "value_columns = [\"IoU\", \"Prior IoU\", \"Acc.\", \"Prior Acc.\"]\n",
    "\n",
    "#display_df.loc[:, \"prior\"] = display_df[\"prior\"].str.replace(\"only_prior\", \"Prior fit only (GT)\")\n",
    "dataset_names = sorted(display_df[\"dataset_name\"].unique())\n",
    "def _order_fnc(x):\n",
    "    if x.name == \"dataset_name\":\n",
    "        return [dataset_names.index(v) for v in x]\n",
    "    if x.name == \"prior\":\n",
    "        return [prior_categories.index(v) for v in x]\n",
    "    return 0\n",
    "\n",
    "#display_df.loc[:, value_columns] = display_df[value_columns].applymap(lambda x: \"{:.3f}\".format(x))\n",
    "display_df = display_df.set_index([\"dataset_name\", \"prior\"]).sort_values(by=[\"dataset_name\", \"prior\"], key=_order_fnc)\n",
    "\n",
    "display_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grps = display_df.groupby(level=\"prior\")\n",
    "# for name, grp in grps:\n",
    "#     print(grp.to_markdown())\n",
    "\n",
    "display_df.groupby(level=\"prior\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out= (\n",
    "        display_df.reset_index()\n",
    "          .rename(columns={'idx1': '', 'idx2': ''})\n",
    "          .to_markdown(tablefmt='github', index=False)\n",
    "      )\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from awesome.util.temporary_property import TemporaryProperty\n",
    "import awesome.run.functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from matplotlib.axes import Axes\n",
    "\n",
    "model: ResultModel = models[0]\n",
    "\n",
    "\n",
    "image_index = 0\n",
    "\n",
    "\n",
    "dataset_names_unique = diffeo_df['dataset_name'].unique().tolist()\n",
    "\n",
    "def qualitative_comparison(dataset_name_idx: int, image_index: int = 0):\n",
    "    dataset_name = dataset_names_unique[dataset_name_idx]\n",
    "    filtered_models_df = df[(df['dataset_name'] == dataset_name)]\n",
    "    if len(filtered_models_df) == 0:\n",
    "        return \"No models found for input combination: dataset_name={}\".format(dataset_name)\n",
    "\n",
    "    display_models =  [x for x in models if x.name in filtered_models_df['index'].values]\n",
    "\n",
    "    size = 10\n",
    "    fig, ax = plt.subplots(1, len(display_models), figsize=(size * len(display_models), size))\n",
    "    \n",
    "    if isinstance(ax, Axes):\n",
    "        ax = [ax]\n",
    "\n",
    "    for i, model in enumerate(display_models):\n",
    "        with TemporaryProperty(model, getitem_mask_mode=\"both\"):\n",
    "            runner = model.get_runner()\n",
    "            dataloader = runner.dataloader\n",
    "            if image_index >= len(dataloader):\n",
    "                image_index = 0\n",
    "                logging.warning(f\"Image index out of bounds for dataset: {dataset_name} len is: {len(dataloader)}. Resetting to 0\")\n",
    "            if len(model) == 0:\n",
    "                logging.warning(f\"No results for model {model.name}. Skipping.\")\n",
    "                continue\n",
    "            res_mask, prior_mask = model[image_index]\n",
    "            image, ground_truth, _input, targets, fg, bg, prior_state = F.prepare_input_eval(dataloader, model, image_index)\n",
    "            \n",
    "            fig = F.plot_image_scribbles(image=image, \n",
    "                                        inference_result=res_mask, \n",
    "                                        foreground_mask=fg, \n",
    "                                        background_mask=bg,\n",
    "                                        prior_result=prior_mask, \n",
    "                                        tight=True, \n",
    "                                        background_value=0,\n",
    "                                        ax=ax[i],\n",
    "                                        )\n",
    "            ax[i].set_title(model.display_name)\n",
    "    return fig\n",
    "\n",
    "ds_widget = widgets.IntSlider(min=0, max=len(dataset_names_unique), value=0, description='Dataset:')\n",
    "index_widget = widgets.IntSlider(min=0, max=100, value=0, description='Image index:')\n",
    "\n",
    "def update_index_range(*args):\n",
    "    # load the dataset\n",
    "    dataset_name = ds_widget.value\n",
    "    models_df = df[(df['dataset_name'] == dataset_name)]\n",
    "    if len(models_df) == 0:\n",
    "        return\n",
    "    m: ResultModel = next((x for x in models if x.name in models_df['index'].values), None)\n",
    "    if m is None:\n",
    "        return\n",
    "    index_widget.max = len(m.get_runner().dataloader) - 1\n",
    "\n",
    "ds_widget.observe(update_index_range, 'value')\n",
    "\n",
    "\n",
    "out = widgets.interact_manual(qualitative_comparison, **{'dataset_name_idx': ds_widget, 'image_index': index_widget})\n",
    "\n",
    "text_widget = widgets.Textarea(\n",
    "    value=\", \".join([str(i) + \": \" +str(x) for i, x in enumerate(dataset_names_unique)]), description=\"Dataset Mapping:\", width=300\n",
    ")\n",
    "\n",
    "#row1 = widgets.HBox([ds_widget, index_widget])\n",
    "row2 = widgets.HBox([text_widget])\n",
    "ui = widgets.VBox([row2])\n",
    "display(ui)\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualitative_comparison(2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(res_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.run.functions import get_result, split_model_result, plot_image_scribbles\n",
    "\n",
    "runner = extract_models[15].get_runner()\n",
    "dataloader = runner.dataloader\n",
    "agent = extract_models[15].get_agent(-1)\n",
    "model = agent._get_model()\n",
    "model_gets_targets = agent.model_gets_targets\n",
    "\n",
    "res, ground_truth, img, fg, bg = get_result(model, dataloader, 10, model_gets_targets=model_gets_targets)\n",
    "res = split_model_result(res, model, dataloader, img)\n",
    "res_prior = res.get(\"prior\", None)\n",
    "res_pred = res[\"segmentation\"]\n",
    "boxes = res.get(\"boxes\", None)\n",
    "labels = res.get(\"labels\", None)\n",
    "\n",
    "p = os.path.join(runner.agent.agent_folder, \"pretrain_priors\")\n",
    "os.makedirs(p, exist_ok=True)\n",
    "\n",
    "iterations = 2000\n",
    "fig = plot_image_scribbles(image=img,\n",
    "                    inference_result=res_pred,\n",
    "                    foreground_mask=fg,\n",
    "                    background_mask=bg,\n",
    "                    prior_result=res_prior,\n",
    "                    save=True,\n",
    "                    path=os.path.join(p, f\"prior_{i}_{iterations}.png\"),\n",
    "                    size=10,\n",
    "                    title=f\"Prior Epoch: {iterations}\", open=True)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting state dict from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "#assert False, \"Stop here\"\n",
    "from collections import OrderedDict\n",
    "extract_models = [x for x in comparison.models if 'REFIT' in x.name and 'edge' in x.name and not 'edgexy' in x.name]\n",
    "pretrain_path = \"./data/checkpoints/refit_unet_uncertainty/{}/model_{}_unet.pth\"\n",
    "date = datetime.now().strftime(\"%y_%m_%d\") \n",
    "for model in extract_models:\n",
    "    datasetname = model.config.name.split(\" \")[1]\n",
    "    agent = model.get_agent(-1)\n",
    "    state_dict = agent._get_model().segmentation_module.state_dict()\n",
    "    os.makedirs(os.path.dirname(pretrain_path.format(date, datasetname)), exist_ok=True)\n",
    "    torch.save(state_dict, pretrain_path.format(date, datasetname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "#assert False, \"Stop here\"\n",
    "\n",
    "extract_models = [x for x in comparison.models if 'REFIT' in x.name and 'edgexy' in x.name]\n",
    "pretrain_path = \"./data/checkpoints/refit_spatial_unet_uncertainty/{}/model_{}_unet.pth\"\n",
    "date = datetime.now().strftime(\"%y_%m_%d\") \n",
    "for model in extract_models:\n",
    "    datasetname = model.config.name.split(\" \")[1]\n",
    "    agent = model.get_agent(-1)\n",
    "    state_dict = agent._get_model().segmentation_module.state_dict()\n",
    "    os.makedirs(os.path.dirname(pretrain_path.format(date, datasetname)), exist_ok=True)\n",
    "    torch.save(state_dict, pretrain_path.format(date, datasetname))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awesome-dC4phDSK-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

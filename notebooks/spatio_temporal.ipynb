{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from awesome.run.awesome_config import AwesomeConfig\n",
    "from awesome.run.awesome_runner import AwesomeRunner\n",
    "from awesome.util.reflection import class_name\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from awesome.dataset.sisbosi_dataset import SISBOSIDataset, ConvexityDataset as SISBOSIConvexityDataset\n",
    "from awesome.dataset.convexity_segmentation_dataset import ConvexitySegmentationDataset\n",
    "from awesome.measures.awesome_loss import AwesomeLoss\n",
    "from awesome.measures.regularizer_loss import RegularizerLoss\n",
    "from awesome.model.convex_diffeomorphism_net import ConvexDiffeomorphismNet\n",
    "from awesome.model.net import Net\n",
    "import awesome\n",
    "from awesome.util.path_tools import get_project_root_path\n",
    "from awesome.util.logging import basic_config\n",
    "import matplotlib.pyplot as plt\n",
    "from awesome.analytics.result_model import ResultModel\n",
    "from awesome.run.functions import get_result, split_model_result, plot_image_scribbles, plot_mask_labels\n",
    "from awesome.util.temporary_property import TemporaryProperty\n",
    "from awesome.run.functions import get_result, split_model_result,register_alpha_map, plot_image_scribbles, plot_mask_labels, plot_mask\n",
    "import numpy as np\n",
    "from matplotlib.colors import to_hex, to_rgb\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from awesome.run.functions import get_mpl_figure\n",
    "from typing import Literal\n",
    "from awesome.dataset.awesome_dataset import AwesomeDataset\n",
    "from awesome.dataset.fbms_sequence_dataset import FBMSSequenceDataset\n",
    "from awesome.dataset.sisbosi_dataset import SISBOSIDataset, ConvexityDataset as SISBOSIConvexityDataset\n",
    "from awesome.measures.awesome_image_loss_joint import AwesomeImageLossJoint\n",
    "from awesome.measures.awesome_image_loss import AwesomeImageLoss\n",
    "from awesome.measures.gradient_penalty_loss import GradientPenaltyLoss\n",
    "from awesome.measures.fbms_joint_loss import FBMSJointLoss\n",
    "from awesome.measures.regularizer_loss import RegularizerLoss\n",
    "from awesome.model.cnn_net import CNNNet\n",
    "from awesome.measures.tv import TV\n",
    "from awesome.model.convex_net import ConvexNet\n",
    "from awesome.model.unet import UNet\n",
    "from awesome.measures.weighted_loss import WeightedLoss\n",
    "from awesome.measures.se import SE\n",
    "from awesome.measures.ae import AE\n",
    "from awesome.measures.unaries_conversion_loss import UnariesConversionLoss\n",
    "from awesome.model.wrapper_module import WrapperModule\n",
    "#load_ext matplotlib\n",
    "#matplotlib tk\n",
    "import normflows as nf\n",
    "basic_config()\n",
    "\n",
    "os.chdir(get_project_root_path()) # Beeing in the root directory of the project is important for the relative paths to work consistently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awesome.model.zoo import Zoo\n",
    "from awesome.model.net_factory import real_nvp_path_connected_net\n",
    "\n",
    "xytype = \"edge\"\n",
    "dataset_kind = \"train\"\n",
    "dataset = \"cars3\"\n",
    "all_frames = True\n",
    "segmentation_model_switch: Literal[\"original\", \"retrain\", \"retrain_xy\"] = \"original\"\n",
    "\n",
    "\n",
    "segmentation_model_state_dict_path = None\n",
    "if segmentation_model_switch == \"original\":\n",
    "    segmentation_model_state_dict_path = f\"./data/checkpoints/labels_with_uncertainty_flownet2_based/model_{dataset}_unet.pth\"\n",
    "elif segmentation_model_switch == \"retrain\":\n",
    "    segmentation_model_state_dict_path = f\"./data/checkpoints/refit_unet_uncertainty/23_11_13/model_{dataset}_unet.pth\"\n",
    "elif segmentation_model_switch == \"retrain_xy\":\n",
    "    segmentation_model_state_dict_path = f\"./data/checkpoints/refit_spatial_unet_uncertainty/23_11_13/model_{dataset}_unet.pth\"\n",
    "else:\n",
    "    raise ValueError(f\"Unknown segmentation_model_switch: {segmentation_model_switch}\")\n",
    "image_channel_format = \"bgr\" if segmentation_model_switch == \"original\" else \"rgb\"\n",
    "input_channels = 4 if xytype == \"edge\" else 6\n",
    "prior_criterion = UnariesConversionLoss(SE(reduction=\"mean\"))\n",
    "channels = 3\n",
    "data_path = f\"./data/local_datasets/FBMS-59/{dataset_kind}/{dataset}\"\n",
    "\n",
    "real_dataset = FBMSSequenceDataset(\n",
    "                    dataset_path=data_path,\n",
    "                    weak_labels_dir = \"weak_labels/labels_with_uncertainty_flownet2_based\",\n",
    "                    processed_weak_labels_dir = \"weak_labels/labels_with_uncertainty_flownet2_based/processed\",\n",
    "                    confidence_dir= \"weak_labels/labels_with_uncertainty_flownet2_based/\",\n",
    "                    do_weak_label_preprocessing=True,\n",
    "                    do_uncertainty_label_flip=True,\n",
    "                    test_weak_label_integrity=True,\n",
    "                    all_frames=True,\n",
    "                )\n",
    "data_path = f\"./data/local_datasets/FBMS-59/{dataset_kind}/{dataset}\"\n",
    "pretrain_checkpoint_dir = f\"./data/checkpoints/pretrain_states/model_{dataset}_unet_spatial_{all_frames}_realnvp_spatio_temporal\"\n",
    "\n",
    "batch_size = 2\n",
    "prior_epochs = 1000\n",
    "prior_reuse_state_epochs = 400\n",
    "prefit_flow_grid_epochs = 30\n",
    "prefit_convex_net_epochs = 400\n",
    "\n",
    "cfg = AwesomeConfig(\n",
    "        name_experiment=f\"UNET+{dataset}+{xytype}+diffeo+spatio-temporal+realnvp\",\n",
    "        dataset_type=class_name(AwesomeDataset),\n",
    "        dataset_args={\n",
    "            \"dataset\": real_dataset,\n",
    "            \"xytype\": xytype,\n",
    "            \"feature_dir\": f\"{data_path}/Feat\",\n",
    "            \"dimension\": \"3d\", # 2d for fcnet\n",
    "            \"mode\": \"model_input\",\n",
    "            \"model_input_requires_grad\": False,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"split_ratio\": 1,\n",
    "            \"shuffle_in_dataloader\": False,\n",
    "            \"image_channel_format\": image_channel_format,\n",
    "            \"do_image_blurring\": True,\n",
    "            \"spatio_temporal\": True,\n",
    "        },\n",
    "        segmentation_model_type=class_name(UNet),\n",
    "        segmentation_model_args={\n",
    "            'in_chn': input_channels,\n",
    "        },\n",
    "        segmentation_training_mode='multi',\n",
    "        segmentation_model_state_dict_path=segmentation_model_state_dict_path, # Path to the pretrained model\n",
    "        use_segmentation_output_inversion=True,\n",
    "        use_prior_model=True,\n",
    "        prior_model_args=dict(\n",
    "            channels=channels,\n",
    "            hidden_units=32,\n",
    "            flow_n_flows=18,\n",
    "            flow_output_fn=\"tanh\",\n",
    "            norm=\"minmax\",\n",
    "            convex_net_hidden_units=130,\n",
    "            convex_net_hidden_layers=2,\n",
    "        ),\n",
    "        prior_model_type=class_name(real_nvp_path_connected_net),\n",
    "        loss_type=class_name(FBMSJointLoss),\n",
    "        loss_args={\n",
    "            \"criterion\": WeightedLoss(torch.nn.BCELoss(), mode=\"sssdms\", noneclass=2),\n",
    "            \"alpha\": 1,\n",
    "            \"beta\": 1,\n",
    "        },\n",
    "        use_extra_penalty_hook=False, # Panalty hook for the panalty term that models output should match\n",
    "        #extra_penalty_after_n_epochs=1,\n",
    "        #use_reduce_lr_in_extra_penalty_hook=False,\n",
    "        use_lr_on_plateau_scheduler=False,\n",
    "        use_binary_classification=True, \n",
    "        num_epochs=0,\n",
    "        device=\"cuda\",\n",
    "        dtype=str(torch.float32),\n",
    "        runs_path=\"./runs/fbms_local/unet/spatio_temporal\",\n",
    "        optimizer_args={\n",
    "            \"lr\": 0.003,\n",
    "            \"betas\": (0.9, 0.999),\n",
    "            \"eps\": 1e-08,\n",
    "            \"amsgrad\": False\n",
    "        },\n",
    "        use_progress_bar=True,\n",
    "        plot_indices_during_training_nth_epoch=20,\n",
    "        plot_indices_during_training=real_dataset.get_ground_truth_indices(),\n",
    "        save_images_after_pretraining=True,\n",
    "        include_unaries_when_saving=True,\n",
    "        agent_args=dict(\n",
    "             do_pretraining=True,\n",
    "             pretrain_only=True, \n",
    "             force_pretrain=True,\n",
    "             pretrain_state_path=pretrain_checkpoint_dir + \".pth\",\n",
    "             pretrain_args=dict(\n",
    "                 use_pretrain_checkpoints=True,\n",
    "                 do_pretrain_checkpoints=True,\n",
    "                 pretrain_checkpoint_dir=pretrain_checkpoint_dir,\n",
    "                 lr=0.001,\n",
    "                 use_logger=True,\n",
    "                 use_step_logger=True,\n",
    "                 num_epochs=prior_epochs,\n",
    "                 proper_prior_fit_retrys=1,\n",
    "                 reuse_state_epochs=prior_reuse_state_epochs,\n",
    "                 # Prefit flow net identity => Flow will be identity(-like) at the beginning\n",
    "                 prefit_flow_net_identity=True,\n",
    "                 prefit_flow_net_identity_lr=1e-2,\n",
    "                 prefit_flow_net_identity_weight_decay=1e-5,\n",
    "                 prefit_flow_net_identity_num_epochs=prefit_flow_grid_epochs,\n",
    "                 # Prefit convex net, to start with a convex thing\n",
    "                 prefit_convex_net=True,\n",
    "                 prefit_convex_net_lr=1e-3,\n",
    "                 prefit_convex_net_weight_decay=0,\n",
    "                 prefit_convex_net_num_epochs=prefit_convex_net_epochs,\n",
    "                 batch_size=batch_size,\n",
    "                 zoo=Zoo()\n",
    "             )\n",
    "        ),\n",
    "        #output_folder=\"./runs/fbms_local/unet/TestUnet/\",\n",
    "    )\n",
    "path = f\"./config/fbms_spatio_temporal/2024_01_16/{cfg.name_experiment}.yaml\"\n",
    "os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "cfg.save_to_file(path, override=True, no_uuid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = AwesomeRunner(cfg)\n",
    "runner.build()\n",
    "runner.store_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from awesome.run.functions import get_mpl_figure, get_result, split_model_result\n",
    "from awesome.model.path_connected_net import PathConnectedNet\n",
    "from typing import Any\n",
    "plt.close(\"all\")\n",
    "\n",
    "grid_shapes = dict()\n",
    "model = runner.agent._get_model()\n",
    "dataloader = runner.dataloader\n",
    "\n",
    "\n",
    "index = list(range(0, len(dataloader), 1))\n",
    "\n",
    "t_n = len(index)\n",
    "t_max = len(dataloader) - 1\n",
    "\n",
    "images = []\n",
    "segmentations = []\n",
    "priors = []\n",
    "\n",
    "for i in index:\n",
    "    res, ground_truth, img, _, _ = get_result(model, dataloader, i, False)\n",
    "    res = split_model_result(res, model, dataloader, img)\n",
    "\n",
    "    res_prior = res.get(\"prior\", None)\n",
    "    res_pred = res[\"segmentation\"]\n",
    "\n",
    "    images.append(img)\n",
    "    segmentations.append(res_pred)\n",
    "    priors.append(res_prior)\n",
    "\n",
    "images = torch.stack(images)\n",
    "segmentations = torch.stack(segmentations)\n",
    "priors = torch.stack(priors)\n",
    "\n",
    "shp = priors.shape[-2:]\n",
    "if shp not in grid_shapes:\n",
    "    grid_shapes[shp] = PathConnectedNet.create_normalized_grid(shp).cpu().numpy()\n",
    "grid = grid_shapes[res_prior.shape[-2:]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Stack time \n",
    "pred = priors # B x C x H x W\n",
    "# Spatio temporal grid\n",
    "t_grid = torch.stack([torch.cat([torch.tensor(grid[0]), torch.full((1, *pred.shape[-2:]), t / t_max)], dim=0) for t in index])\n",
    "\n",
    "\n",
    "\n",
    "def plot_spatio_temporal_object(grid: Any, unaries: Any, size: float = 5):\n",
    "    \n",
    "    if isinstance(grid, torch.Tensor):\n",
    "        grid = grid.cpu().numpy()\n",
    "    if isinstance(unaries, torch.Tensor):\n",
    "        unaries = unaries.cpu().numpy()\n",
    "    \n",
    "    if len(grid.shape) < 4:\n",
    "        grid = grid[None]\n",
    "    if len(unaries.shape) < 4:\n",
    "        unaries = unaries[None]\n",
    "    \n",
    "\n",
    "    fig, ax = get_mpl_figure(subplot_kw=dict(projection='3d'))\n",
    "\n",
    "    for i in range(grid.shape[0]):\n",
    "        g = grid[i]\n",
    "        u = unaries[i][0]\n",
    "\n",
    "        z = u\n",
    "        y = g[1]\n",
    "        x = g[0]\n",
    "        offset = g[2].max() # Offset is the time\n",
    "        ax.contour(x, y, z, levels=[0.5], colors=\"red\", offset=offset, linewidths=2)\n",
    "\n",
    "    x_left, x_right = ax.get_xlim()\n",
    "    y_low, y_high = ax.get_ylim()\n",
    "\n",
    "    zoom= 1\n",
    "    elevation = 130\n",
    "    azimuth = 90\n",
    "    roll = 0\n",
    "\n",
    "    ax.set_box_aspect(aspect=((x_right-x_left)/(y_low-y_high), 1, 1), zoom=zoom)\n",
    "    ax.view_init(elev=elevation, azim=azimuth, roll=roll)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    return fig\n",
    "\n",
    "fig = plot_spatio_temporal_object(t_grid, pred)\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from awesome.run.functions import plot_as_image\n",
    "\n",
    "\n",
    "p = pred[0][0]\n",
    "\n",
    "all_contours = []\n",
    "all_hierarchy = []\n",
    "\n",
    "times = [t / t_max for t in index]\n",
    "\n",
    "for i in range(0, len(pred)):\n",
    "    p = pred[i][0]\n",
    "    t = times[i]\n",
    "    ret, thresh = cv.threshold(((1 - p.numpy()) * 255).astype(np.uint8), 123, 1, cv.THRESH_BINARY)\n",
    "    contours, hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # We are loosing some information here, eg. holes in the object are not represented\n",
    "    # This is because we are only using the contour of the object\n",
    "\n",
    "    local_contours = [np.concatenate([c[:, 0, :] / (torch.tensor(p.shape[-2:]).numpy() - 1), np.full((c.shape[0], 1), t)], axis=1) for c in contours]\n",
    "    all_contours.extend(local_contours)\n",
    "    all_hierarchy.extend(hierarchy[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyvista as pv\n",
    "\n",
    "points = np.concatenate(all_contours)\n",
    "cloud = pv.PolyData(points)\n",
    "cloud.plot()\n",
    "\n",
    "volume = cloud.delaunay_3d(alpha=1)\n",
    "shell = volume.extract_geometry()\n",
    "\n",
    "axes = pv.Axes()\n",
    "display(axes.show_actor())\n",
    "\n",
    "shell.plot(show_axes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install trame-vuetify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = get_mpl_figure()\n",
    "\n",
    "x = contours[1].squeeze()[:, 0]\n",
    "y = contours[1].squeeze()[:, 1]\n",
    "ax.plot(x, y, color=\"red\", linewidth=2)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours[0].squeeze()[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.linspace(0, 1, 5)\n",
    "torch.cat([res, torch.full(t)], dim=0) for t in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Experiments with glow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xytype = \"edge\"\n",
    "dataset_kind = \"train\"\n",
    "dataset = \"bear01\"\n",
    "all_frames = True\n",
    "subset = None #slice(0, 5)\n",
    "segmentation_model_switch: Literal[\"original\", \"retrain\", \"retrain_xy\"] = \"original\"\n",
    "\n",
    "\n",
    "segmentation_model_state_dict_path = None\n",
    "if segmentation_model_switch == \"original\":\n",
    "    segmentation_model_state_dict_path = f\"./data/checkpoints/labels_with_uncertainty_flownet2_based/model_{dataset}_unet.pth\"\n",
    "elif segmentation_model_switch == \"retrain\":\n",
    "    segmentation_model_state_dict_path = f\"./data/checkpoints/refit_unet_uncertainty/23_11_13/model_{dataset}_unet.pth\"\n",
    "elif segmentation_model_switch == \"retrain_xy\":\n",
    "    segmentation_model_state_dict_path = f\"./data/checkpoints/refit_spatial_unet_uncertainty/23_11_13/model_{dataset}_unet.pth\"\n",
    "else:\n",
    "    raise ValueError(f\"Unknown segmentation_model_switch: {segmentation_model_switch}\")\n",
    "image_channel_format = \"bgr\" if segmentation_model_switch == \"original\" else \"rgb\"\n",
    "\n",
    "prior_criterion = UnariesConversionLoss(SE(reduction=\"mean\"))\n",
    "data_path = f\"./data/local_datasets/FBMS-59/{dataset_kind}/{dataset}\"\n",
    "\n",
    "real_dataset = FBMSSequenceDataset(\n",
    "                    dataset_path=data_path,\n",
    "                    weak_labels_dir = \"weak_labels/labels_with_uncertainty_flownet2_based\",\n",
    "                    processed_weak_labels_dir = \"weak_labels/labels_with_uncertainty_flownet2_based/processed\",\n",
    "                    confidence_dir= \"weak_labels/labels_with_uncertainty_flownet2_based/\",\n",
    "                    do_weak_label_preprocessing=True,\n",
    "                    do_uncertainty_label_flip=True,\n",
    "                    test_weak_label_integrity=False,\n",
    "                    all_frames=True,\n",
    "                )\n",
    "dataset = AwesomeDataset(\n",
    "    **{\n",
    "        \"dataset\": real_dataset,\n",
    "        \"xytype\": xytype,\n",
    "        \"feature_dir\": f\"{data_path}/Feat\",\n",
    "        \"dimension\": \"3d\", # 2d for fcnet\n",
    "        \"mode\": \"model_input\",\n",
    "        \"model_input_requires_grad\": False,\n",
    "        \"batch_size\": 1,\n",
    "        \"split_ratio\": 1,\n",
    "        \"shuffle_in_dataloader\": False,\n",
    "        \"image_channel_format\": image_channel_format,\n",
    "        \"do_image_blurring\": True,\n",
    "        \"model_input_requires_grad\": True,\n",
    "        \"subset\": subset,\n",
    "        \"spatio_temporal\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "segmentation_model = UNet(4, 1)\n",
    "segmentation_model.load_state_dict(torch.load(segmentation_model_state_dict_path))\n",
    "\n",
    "\n",
    "def init_glow(channels: int, \n",
    "              hidden_channels: int,\n",
    "              n_flows: int,\n",
    "              height: int, \n",
    "              width: int,\n",
    "              scale: bool = True,\n",
    "              scale_map: Literal[\"sigmoid\", \"exp\"] = \"sigmoid\",\n",
    "              ) -> nf.NormalizingFlow:\n",
    "    # Define flows\n",
    "\n",
    "    input_shape = (channels, height, width)\n",
    "\n",
    "    # Set up flows, distributions and merge operations\n",
    "    q0 = nf.distributions.base.Uniform(input_shape, 0, 1)\n",
    "    flows = []\n",
    "    \n",
    "    for j in range(n_flows):\n",
    "        flows += [nf.flows.GlowBlock(channels, hidden_channels,\n",
    "                                    split_mode='channel', \n",
    "                                    scale_map=scale_map, leaky=0.01,\n",
    "                                    scale=scale, net_actnorm=False)]\n",
    "\n",
    "    # Construct flow model with the multiscale architecture\n",
    "    model = nf.NormalizingFlow(q0, \n",
    "                               flows, \n",
    "                               q0)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from awesome.agent.torch_agent import TorchAgent\n",
    "from awesome.dataset.prior_dataset import PriorManager\n",
    "from awesome.measures.unaries_weighted_loss import UnariesWeightedLoss\n",
    "from awesome.model.wrapper_module import WrapperModule\n",
    "from awesome.model.unet import UNet\n",
    "from awesome.model.path_connected_net import PathConnectedNet\n",
    "from awesome.model.convex_net import ConvexNextNet\n",
    "from normflows import NormalizingFlow\n",
    "from normflows.flows import GlowBlock\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from awesome.util.torch import TensorUtil\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "channels = 2\n",
    "\n",
    "image_shape = dataset[0][0][0].shape[1:]\n",
    "\n",
    "flow_model = init_glow(channels=channels, hidden_channels=256, n_flows=3, \n",
    "                       height=image_shape[0], \n",
    "                       width=image_shape[1],\n",
    "                       scale=True)\n",
    "convex_model = ConvexNextNet(n_hidden=130, \n",
    "                             n_hidden_layers=2,\n",
    "                             in_features=channels)\n",
    "\n",
    "path_connected_model = PathConnectedNet(convex_model, flow_model)\n",
    "\n",
    "wrapper_module = WrapperModule(\n",
    "    segmentation_module=segmentation_model,\n",
    "    prior_module=path_connected_model,\n",
    "    prior_arg_mode=\"param_clean_grid\"\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-7\n",
    "\n",
    "previous_state = None\n",
    "previous_center_of_mass = None\n",
    "\n",
    "prior_module = wrapper_module.prior_module\n",
    "\n",
    "\n",
    "use_prior_sigmoid = True\n",
    "use_logger = False\n",
    "use_step_logger = False\n",
    "batch_progress_bar = None\n",
    "\n",
    "criterion = UnariesConversionLoss(SE(reduction=\"mean\"))\n",
    "\n",
    "TensorUtil.to(wrapper_module, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels, indices, prior_state = TorchAgent.decompose_training_item(dataset[0], training_dataset=dataset)\n",
    "\n",
    "max_iter = 1000\n",
    "\n",
    "loss_hist = np.array([])\n",
    "\n",
    "grid = inputs[2]\n",
    "\n",
    "prior_module = wrapper_module.prior_module\n",
    "model = prior_module.flow_net\n",
    "\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "inputs, labels, indices, prior_state = TorchAgent.decompose_training_item(dataset[0], training_dataset=dataset)\n",
    "\n",
    "grid = inputs[2]\n",
    "\n",
    "grid = grid.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "grid = grid[None,...]\n",
    "\n",
    "model.train()\n",
    "\n",
    "for i in tqdm(range(max_iter)):\n",
    "    \n",
    "    x, y = grid, grid\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = model.forward_kld(x.to(device))\n",
    "        \n",
    "    if ~(torch.isnan(loss) | torch.isinf(loss)):\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss_hist = np.append(loss_hist, loss.detach().to('cpu').numpy())\n",
    "    del(x, y, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "it = data_loader\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_progress_bar = True\n",
    "\n",
    "if use_progress_bar:\n",
    "    it = tqdm(it, desc=\"Pretraining images\")\n",
    "\n",
    "for i, item in enumerate(it):\n",
    "    inputs, labels, indices, prior_state = TorchAgent.decompose_training_item(item, training_dataset=dataset)\n",
    "    device_inputs: torch.Tensor = TensorUtil.to(\n",
    "        inputs, device=device)\n",
    "    # device_labels: torch.Tensor = TensorUtil.to(labels, device=device)\n",
    "\n",
    "    # Evaluate model to get unaries\n",
    "    # Switch prior weights if needed, using context manager\n",
    "    with PriorManager(wrapper_module,\n",
    "                        prior_state=prior_state,\n",
    "                        prior_cache=dataset.__prior_cache__,\n",
    "                        model_device=device,\n",
    "                        training=True\n",
    "                        ):\n",
    "        \n",
    "        unaries = None\n",
    "        has_proper_prior_fit = False\n",
    "        loaded_current_from_checkpoint = False\n",
    "\n",
    "        # Get the unaries\n",
    "        # Disable prior evaluation to just get the unaries\n",
    "        with torch.no_grad(), TemporaryProperty(wrapper_module, evaluate_prior=False):\n",
    "            if isinstance(device_inputs, list):\n",
    "                unaries = wrapper_module(*device_inputs)\n",
    "            else:\n",
    "                unaries = wrapper_module(device_inputs)\n",
    "\n",
    "\n",
    "        # Getting inputs for prior\n",
    "        prior_args, prior_kwargs = wrapper_module.get_prior_args(device_inputs[0],\n",
    "                                                                    *device_inputs[1:],\n",
    "                                                                    segm=unaries[0, ...],\n",
    "                                                                    )\n",
    "        _input = prior_args[0]\n",
    "        actual_input = _input.detach().clone()\n",
    "\n",
    "        _unique_vals = torch.unique(unaries >= 0.5)\n",
    "        # Check if unaries output contains at least some foreground\n",
    "        if len(_unique_vals) == 1:\n",
    "            # No foreground / background predicted. Skip this image\n",
    "            # We will keep the state of the prior if reuse_state is True\n",
    "            # If there was a pre existing state, we will use it again\n",
    "            logging.warning(f\"Unaries of segmentation model contain no foreground. Skipping image. {i}\")\n",
    "            continue\n",
    "        \n",
    "        # Determine number of epochs\n",
    "        epochs = num_epochs\n",
    "        \n",
    "        # Train n iterations\n",
    "        it = range(epochs)\n",
    "        if use_progress_bar:\n",
    "            desc = f'Image {i + 1}: Pretraining'\n",
    "            if batch_progress_bar is None:\n",
    "                batch_progress_bar = tqdm(\n",
    "                    total=epochs,\n",
    "                    desc=desc,\n",
    "                    leave=True)\n",
    "            else:\n",
    "                batch_progress_bar.reset(total=epochs)\n",
    "                batch_progress_bar.set_description(desc)\n",
    "\n",
    "        groups = []\n",
    "        groups += [dict(params=prior_module.flow_net.parameters(), weight_decay=weight_decay)]\n",
    "        groups += [dict(params=prior_module.convex_net.parameters())]\n",
    "        \n",
    "        optimizer = torch.optim.Adam(groups, lr=lr)\n",
    "\n",
    "        device_prior_output = None\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # Train n iterations\n",
    "            for step in it:\n",
    "                optimizer.zero_grad()\n",
    "                # Forward pass\n",
    "                device_prior_output = prior_module(actual_input, *prior_args[1:], **prior_kwargs)\n",
    "                device_prior_output = wrapper_module.process_prior_output(\n",
    "                    device_prior_output, use_sigmoid=use_prior_sigmoid)[None, ...]  # Add batch dim again\n",
    "\n",
    "                loss: torch.Tensor = criterion(\n",
    "                    device_prior_output, unaries)\n",
    "\n",
    "                if ~(torch.isnan(loss) | torch.isinf(loss)):\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    logging.warning(\n",
    "                        f\"Loss is nan or inf. Skipping step {step} of image {i}\")\n",
    "                    break\n",
    "\n",
    "                if use_logger and use_step_logger:\n",
    "                    logger.log_value(\n",
    "                        loss.item(), f\"PretrainingLoss/Image_{i}\", step=step)\n",
    "\n",
    "                prior_module.enforce_convexity()\n",
    "                if batch_progress_bar is not None:\n",
    "                    batch_progress_bar.set_postfix(\n",
    "                        loss=loss.item(), refresh=False)\n",
    "                    batch_progress_bar.update()\n",
    "                        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid = inputs[2]\n",
    "\n",
    "prior_module = wrapper_module.prior_module\n",
    "norm_flow = prior_module.flow_net\n",
    "\n",
    "with torch.no_grad():\n",
    "    norm_flow.eval()\n",
    "    grid = grid.to(device)\n",
    "    grid = grid[None, ...]\n",
    "    out_grid = norm_flow(grid)\n",
    "\n",
    "\n",
    "out_grid.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, ground_truth, img, fg, bg = get_result(wrapper_module, dataset, 0, model_gets_targets=False)\n",
    "res = split_model_result(res, wrapper_module, dataset, img, compute_crf=False)\n",
    "res_prior = res.get(\"prior\", None)\n",
    "res_pred = res[\"segmentation\"]\n",
    "fig = plot_image_scribbles(img, res_pred, fg, bg, res_prior, save=True, size=5, tight_layout=True, title=\"Epoch: \" + str(step),\n",
    "                                        legend=False)\n",
    "fig                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_module"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awesome-dC4phDSK-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from awesome.model.unet import UNet\n",
    "from awesome.dataset.awesome_dataset import AwesomeDataset\n",
    "from awesome.dataset.fbms_sequence_dataset import FBMSSequenceDataset\n",
    "from awesome.util.path_tools import get_project_root_path\n",
    "import os\n",
    "from awesome.run.functions import plot_as_image, channel_masks_to_value_mask, transparent_added_listed_colormap, get_mpl_figure, plot_mask\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "os.chdir(get_project_root_path())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_kind = \"train\"\n",
    "dataset = \"ducks01\"\n",
    "data_path = f\"./data/local_datasets/FBMS-59/{dataset_kind}/{dataset}\"\n",
    "fbms_ds = FBMSSequenceDataset(\n",
    "                        dataset_path=data_path,\n",
    "                        weak_labels_dir = \"weak_labels/labels_with_uncertainty_flownet2_based_new\",\n",
    "                        processed_weak_labels_dir = \"weak_labels/labels_with_uncertainty_flownet2_based/processed\",\n",
    "                        confidence_dir= \"weak_labels/labels_with_uncertainty_flownet2_based/\",\n",
    "                        do_weak_label_preprocessing=False,\n",
    "                        do_uncertainty_label_flip=False,\n",
    "                        all_frames=True,\n",
    "                        test_weak_label_integrity=False,\n",
    "                        label_mode=\"multiple_objects\",\n",
    "                        segmentation_object_id=[]\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No valid foreground weak label object id for sample ducks01:0!\n",
      "WARNING:root:No valid foreground weak label object id for sample ducks01:19!\n",
      "WARNING:root:No valid foreground weak label object id for sample ducks01:39!\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from awesome.measures.fbms_joint_loss import FBMSJointLoss\n",
    "from awesome.measures.se import SE\n",
    "from awesome.measures.unaries_conversion_loss import UnariesConversionLoss\n",
    "from awesome.measures.weighted_loss import WeightedLoss\n",
    "from awesome.model.batch_size_multi_prior_module import BatchSizeMultiPriorModule\n",
    "from awesome.model.combined_segmentation_module import CombinedSegmentationModule\n",
    "from awesome.model.multiple_object_aware_path_connected_net import MultipleObjectsAwarePathConnectedNet\n",
    "from awesome.model.number_based_multi_prior_module import NumberBasedMultiPriorModule\n",
    "from awesome.model.zoo import Zoo\n",
    "from awesome.model.net_factory import real_nvp_path_connected_net\n",
    "from awesome.run.awesome_config import AwesomeConfig\n",
    "from awesome.util.reflection import class_name\n",
    "\n",
    "xytype = \"edge\"\n",
    "dataset_kind = \"train\"\n",
    "dataset = \"ducks01\"\n",
    "all_frames = True\n",
    "prior_epochs = 4000\n",
    "prior_refit_epochs = 400\n",
    "subset = None # 0 #slice(0, 5)\n",
    "segmentation_model_switch: Literal[\"original\", \"retrain\", \"retrain_xy\"] = \"original\"\n",
    "segmentation_model_state_dict_path = None\n",
    "    \n",
    "if segmentation_model_switch == \"original\":\n",
    "    segmentation_model_state_dict_path = f\"./data/checkpoints/labels_with_uncertainty_flownet2_based/model_{dataset}_unet.pth\"\n",
    "elif segmentation_model_switch == \"retrain\":\n",
    "    segmentation_model_state_dict_path = f\"./data/checkpoints/refit_unet_uncertainty/23_11_13/model_{dataset}_unet.pth\"\n",
    "elif segmentation_model_switch == \"retrain_xy\":\n",
    "    segmentation_model_state_dict_path = f\"./data/checkpoints/refit_spatial_unet_uncertainty/23_11_13/model_{dataset}_unet.pth\"\n",
    "else:\n",
    "    raise ValueError(f\"Unknown segmentation_modevl_switch: {segmentation_model_switch}\")\n",
    "image_channel_format = \"bgr\" if segmentation_model_switch == \"original\" else \"rgb\"\n",
    "input_channels = 4 if xytype == \"edge\" else 6\n",
    "\n",
    "prior_criterion = UnariesConversionLoss(SE(reduction=\"mean\"))\n",
    "data_path = f\"./data/local_datasets/FBMS-59/{dataset_kind}/{dataset}\"\n",
    "\n",
    "pretrain_state_path = f\"./data/checkpoints/pretrain_states/2024-01-11/model_{dataset}_unet_spatial_realnvp_{prior_epochs}_{prior_refit_epochs}_multi_prior_testing\"\n",
    "\n",
    "\n",
    "real_dataset = FBMSSequenceDataset(\n",
    "                dataset_path=data_path,\n",
    "                weak_labels_dir = \"weak_labels/labels_with_uncertainty_flownet2_based_new\",\n",
    "                processed_weak_labels_dir = \"weak_labels/labels_with_uncertainty_flownet2_based/processed\",\n",
    "                confidence_dir= \"weak_labels/labels_with_uncertainty_flownet2_based/\",\n",
    "                do_weak_label_preprocessing=False,\n",
    "                do_uncertainty_label_flip=False,\n",
    "                test_weak_label_integrity=False,\n",
    "                all_frames=True,\n",
    "                label_mode=\"multiple_objects\",\n",
    "                segmentation_object_id=[]\n",
    "            )\n",
    "real_dataset.test_weak_label_integrity = True\n",
    "prior_epochs = 4000\n",
    "prior_refit_epochs = 400\n",
    "segmentation_model_switch: Literal[\"original\", \"retrain\", \"retrain_xy\"] = \"original\"\n",
    "number_of_objects = real_dataset.get_number_of_objects()\n",
    "\n",
    "inner_prior_factory = real_nvp_path_connected_net\n",
    "inner_prior_factory_model_args=dict(\n",
    "        channels=2,\n",
    "        hidden_units=32,\n",
    "        flow_n_flows=12,\n",
    "        flow_output_fn=\"tanh\",\n",
    "        norm=\"minmax\",\n",
    "        convex_net_hidden_units=130,\n",
    "        convex_net_hidden_layers=2,\n",
    "        network_type=MultipleObjectsAwarePathConnectedNet,\n",
    "    )\n",
    "\n",
    "prior_type, prior_args = BatchSizeMultiPriorModule.get_type_args(*NumberBasedMultiPriorModule.get_type_args(\n",
    "    inner_prior_factory, inner_prior_factory_model_args\n",
    "))\n",
    "\n",
    "cfg = AwesomeConfig(\n",
    "    name_experiment=f\"UNET+nbatch+multiobject+testing\",\n",
    "    combined_segmentation_module_type=class_name(CombinedSegmentationModule),\n",
    "    combined_segmentation_module_args=dict(),\n",
    "    dataset_type=class_name(AwesomeDataset),\n",
    "    dataset_args={\n",
    "        \"dataset\": real_dataset,\n",
    "        \"xytype\": xytype,\n",
    "        \"feature_dir\": f\"{data_path}/Feat\",\n",
    "        \"dimension\": \"3d\", # 2d for fcnet\n",
    "        \"mode\": \"model_input\",\n",
    "        \"model_input_requires_grad\": False,\n",
    "        \"batch_size\": 1,\n",
    "        \"split_ratio\": 1,\n",
    "        \"shuffle_in_dataloader\": False,\n",
    "        \"image_channel_format\": image_channel_format,\n",
    "        \"do_image_blurring\": True\n",
    "    },\n",
    "    segmentation_model_type=class_name(UNet),\n",
    "    segmentation_model_args={\n",
    "        'in_chn': input_channels,\n",
    "        'out_chn': number_of_objects if number_of_objects == 1 else number_of_objects + 1, # If single object we use BCE, if multiple we use CE and neet backgorund channel\n",
    "    },\n",
    "    segmentation_training_mode='multi',\n",
    "    segmentation_model_state_dict_path=segmentation_model_state_dict_path, # Path to the pretrained model\n",
    "    use_segmentation_output_inversion=True,\n",
    "    use_prior_model=True,\n",
    "    prior_model_args=prior_args,\n",
    "    prior_model_type=prior_type,\n",
    "    loss_type=class_name(FBMSJointLoss),\n",
    "    loss_args={\n",
    "        \"criterion\": WeightedLoss(torch.nn.BCELoss(), mode=\"sssdms\", noneclass=2),\n",
    "        \"penalty_criterion\": prior_criterion.criterion,\n",
    "        \"alpha\": 1,\n",
    "        \"beta\": 1,\n",
    "    },\n",
    "    use_extra_penalty_hook=False, # Panalty hook for the panalty term that models output should match\n",
    "    #extra_penalty_after_n_epochs=1,\n",
    "    #use_reduce_lr_in_extra_penalty_hook=False,\n",
    "    use_lr_on_plateau_scheduler=False,\n",
    "    use_binary_classification=True, \n",
    "    num_epochs=0,\n",
    "    device=\"cuda\",\n",
    "    dtype=str(torch.float32),\n",
    "    runs_path=\"./runs/fbms_local/unet/multi_prior\",\n",
    "    optimizer_args={\n",
    "        \"lr\": 0.003,\n",
    "        \"betas\": (0.9, 0.999),\n",
    "        \"eps\": 1e-08,\n",
    "        \"amsgrad\": False\n",
    "    },\n",
    "    use_progress_bar=False,\n",
    "    plot_indices_during_training=real_dataset.get_ground_truth_indices(),\n",
    "    save_images_after_pretraining=True,\n",
    "    include_unaries_when_saving=True,\n",
    "    agent_args=dict(\n",
    "            do_pretraining=True,\n",
    "            pretrain_only=True, \n",
    "            force_pretrain=True,\n",
    "            pretrain_state_path=pretrain_state_path + \".pth\",\n",
    "            pretrain_args=dict(\n",
    "                use_pretrain_checkpoints=True,\n",
    "                do_pretrain_checkpoints=True,\n",
    "                pretrain_checkpoint_dir=pretrain_state_path,\n",
    "                lr=0.001,\n",
    "                use_logger=True,\n",
    "                use_step_logger=True,\n",
    "                num_epochs=4000,\n",
    "                proper_prior_fit_retrys=1,\n",
    "                reuse_state_epochs=400,\n",
    "                # Prefit flow net identity => Flow will be identity(-like) at the beginning\n",
    "                prefit_flow_net_identity=True,\n",
    "                prefit_flow_net_identity_lr=1e-2,\n",
    "                prefit_flow_net_identity_weight_decay=1e-5,\n",
    "                prefit_flow_net_identity_num_epochs=100,\n",
    "                # Prefit convex net, to start with a convex thing\n",
    "                prefit_convex_net=True,\n",
    "                prefit_convex_net_lr=1e-3,\n",
    "                prefit_convex_net_weight_decay=0,\n",
    "                prefit_convex_net_num_epochs=200,\n",
    "                zoo=Zoo()\n",
    "            )\n",
    "    )\n",
    ")\n",
    "#cfg.save_to_file(f\"./config/{cfg.name_experiment}.yaml\", override=True, no_uuid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m runner \u001b[38;5;241m=\u001b[39m AwesomeRunner(cfg)\n\u001b[0;32m      5\u001b[0m runner\u001b[38;5;241m.\u001b[39mbuild()\n\u001b[1;32m----> 6\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\projects\\AWESOME\\awesome\\run\\awesome_runner.py:543\u001b[0m, in \u001b[0;36mAwesomeRunner.train\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 543\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mtrain(num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_epochs,\n\u001b[0;32m    544\u001b[0m                      tqdm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_progress_bar, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\projects\\AWESOME\\awesome\\agent\\torch_agent.py:673\u001b[0m, in \u001b[0;36mTorchAgent.train\u001b[1;34m(self, num_epochs, keep_device, **kwargs)\u001b[0m\n\u001b[0;32m    670\u001b[0m use_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress_bar)\n\u001b[0;32m    672\u001b[0m \u001b[38;5;66;03m# Pretraining if wanted:\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pretrain(model, train_set\u001b[38;5;241m=\u001b[39mtrain_set, test_set\u001b[38;5;241m=\u001b[39mtest_set,\n\u001b[0;32m    674\u001b[0m                use_progress_bar\u001b[38;5;241m=\u001b[39muse_progress_bar, keep_device\u001b[38;5;241m=\u001b[39mkeep_device, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrain_only \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrain_only \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrain_only\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))):\n\u001b[0;32m    679\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPretraining done, exiting...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\projects\\AWESOME\\awesome\\agent\\torch_agent.py:588\u001b[0m, in \u001b[0;36mTorchAgent._pretrain\u001b[1;34m(self, model, train_set, test_set, use_progress_bar, keep_device, **kwargs)\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state_loaded \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce_pretrain \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce_pretrain \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_pretrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))):\n\u001b[0;32m    587\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting pretraining...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 588\u001b[0m     state \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpretrain(\n\u001b[0;32m    589\u001b[0m         train_set\u001b[38;5;241m=\u001b[39mtrain_set,\n\u001b[0;32m    590\u001b[0m         test_set\u001b[38;5;241m=\u001b[39mtest_set,\n\u001b[0;32m    591\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m    592\u001b[0m         agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    593\u001b[0m         use_progress_bar\u001b[38;5;241m=\u001b[39muse_progress_bar,\n\u001b[0;32m    594\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpretraining_kwargs)\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Save the state\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\projects\\AWESOME\\awesome\\model\\wrapper_module.py:353\u001b[0m, in \u001b[0;36mWrapperModule.pretrain\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior_module, PretrainableModule):\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrior module must be a PretrainableModule\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior_module\u001b[38;5;241m.\u001b[39mpretrain(\u001b[38;5;241m*\u001b[39margs, \n\u001b[0;32m    354\u001b[0m wrapper_module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    355\u001b[0m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\projects\\AWESOME\\awesome\\model\\abstract_multi_prior_module.py:105\u001b[0m, in \u001b[0;36mAbstractMultiPriorModule.pretrain\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpriors[\u001b[38;5;241m0\u001b[39m], PretrainableModule):\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrior module must be a PretrainableModule\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpriors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpretrain(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\projects\\AWESOME\\awesome\\model\\abstract_multi_prior_module.py:105\u001b[0m, in \u001b[0;36mAbstractMultiPriorModule.pretrain\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpriors[\u001b[38;5;241m0\u001b[39m], PretrainableModule):\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrior module must be a PretrainableModule\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpriors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpretrain(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\projects\\AWESOME\\awesome\\model\\path_connected_net.py:488\u001b[0m, in \u001b[0;36mPathConnectedNet.pretrain\u001b[1;34m(self, train_set, test_set, device, agent, use_progress_bar, do_pretrain_checkpoints, use_pretrain_checkpoints, pretrain_checkpoint_dir, wrapper_module, **kwargs)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mawesome\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprior_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PriorDataset\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(agent\u001b[38;5;241m.\u001b[39mtraining_dataset, PriorDataset) \u001b[38;5;129;01mand\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mtraining_dataset\u001b[38;5;241m.\u001b[39mhas_prior:\n\u001b[0;32m    487\u001b[0m     \u001b[38;5;66;03m# Prior based pretraining (per image)\u001b[39;00m\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prior_based_pretrain(train_set\u001b[38;5;241m=\u001b[39mtrain_set,\n\u001b[0;32m    489\u001b[0m                                         test_set\u001b[38;5;241m=\u001b[39mtest_set,\n\u001b[0;32m    490\u001b[0m                                         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m    491\u001b[0m                                         agent\u001b[38;5;241m=\u001b[39magent,\n\u001b[0;32m    492\u001b[0m                                         use_progress_bar\u001b[38;5;241m=\u001b[39muse_progress_bar,\n\u001b[0;32m    493\u001b[0m                                         do_pretrain_checkpoints\u001b[38;5;241m=\u001b[39mdo_pretrain_checkpoints,\n\u001b[0;32m    494\u001b[0m                                         use_pretrain_checkpoints\u001b[38;5;241m=\u001b[39muse_pretrain_checkpoints,\n\u001b[0;32m    495\u001b[0m                                         pretrain_checkpoint_dir\u001b[38;5;241m=\u001b[39mpretrain_checkpoint_dir,\n\u001b[0;32m    496\u001b[0m                                         wrapper_module\u001b[38;5;241m=\u001b[39mwrapper_module,\n\u001b[0;32m    497\u001b[0m                                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;66;03m# Spatio Temporality based pretraining\u001b[39;00m\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_prior_based_pretrain(train_set\u001b[38;5;241m=\u001b[39mtrain_set,\n\u001b[0;32m    501\u001b[0m                                           test_set\u001b[38;5;241m=\u001b[39mtest_set,\n\u001b[0;32m    502\u001b[0m                                         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    508\u001b[0m                                         wrapper_module\u001b[38;5;241m=\u001b[39mwrapper_module,\n\u001b[0;32m    509\u001b[0m                                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\projects\\AWESOME\\awesome\\model\\multiple_object_aware_path_connected_net.py:114\u001b[0m, in \u001b[0;36mMultipleObjectsAwarePathConnectedNet._prior_based_pretrain\u001b[1;34m(self, train_set, test_set, device, agent, use_progress_bar, do_pretrain_checkpoints, use_pretrain_checkpoints, pretrain_checkpoint_dir, wrapper_module, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m device_inputs: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m TensorUtil\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m    109\u001b[0m     inputs, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# device_labels: torch.Tensor = TensorUtil.to(labels, device=device)\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Evaluate model to get unaries\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Switch prior weights if needed, using context manager\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PriorManager(wrapper_module,\n\u001b[0;32m    115\u001b[0m                   prior_state\u001b[38;5;241m=\u001b[39mprior_state,\n\u001b[0;32m    116\u001b[0m                   prior_cache\u001b[38;5;241m=\u001b[39magent\u001b[38;5;241m.\u001b[39mtraining_dataset\u001b[38;5;241m.\u001b[39m__prior_cache__,\n\u001b[0;32m    117\u001b[0m                   model_device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m    118\u001b[0m                   training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    119\u001b[0m                   ):\n\u001b[0;32m    121\u001b[0m     unaries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     has_proper_prior_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mD:\\projects\\AWESOME\\awesome\\dataset\\prior_dataset.py:61\u001b[0m, in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     58\u001b[0m if not has_prior:\n\u001b[0;32m     59\u001b[0m     return inner_collate_fn(batch)\n\u001b[0;32m     60\u001b[0m else:\n\u001b[1;32m---> 61\u001b[0m     # Extract the state and return it as list, while the rest is handled by the inner collate function.\n\u001b[0;32m     62\u001b[0m     states = []\n\u001b[0;32m     63\u001b[0m     items = []\n",
      "File \u001b[1;32mD:\\projects\\AWESOME\\awesome\\dataset\\prior_dataset.py:61\u001b[0m, in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     58\u001b[0m if not has_prior:\n\u001b[0;32m     59\u001b[0m     return inner_collate_fn(batch)\n\u001b[0;32m     60\u001b[0m else:\n\u001b[1;32m---> 61\u001b[0m     # Extract the state and return it as list, while the rest is handled by the inner collate function.\n\u001b[0;32m     62\u001b[0m     states = []\n\u001b[0;32m     63\u001b[0m     items = []\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Schneider\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\awesome-dC4phDSK-py3.9\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Schneider\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\awesome-dC4phDSK-py3.9\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from awesome.run.awesome_runner import AwesomeRunner\n",
    "\n",
    "\n",
    "runner = AwesomeRunner(cfg)\n",
    "runner.build()\n",
    "runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.__runner_context__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awesome-dC4phDSK-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
